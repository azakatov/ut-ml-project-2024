{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/azakatov/ut-ml-project-2024/blob/try-ensemble/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "img-main-button"
   },
   "source": [
    "<svg width=\"416.16\" height=\"24.064\" viewBox=\"0 0 416.16 24.064\" xmlns=\"http://www.w3.org/2000/svg\"><g id=\"svgGroup\" stroke-linecap=\"round\" fill-rule=\"evenodd\" font-size=\"9pt\" stroke=\"#ff0000\" stroke-width=\"0.25mm\" fill=\"#ff0000\" style=\"stroke:#ff0000;stroke-width:0.25mm;fill:#ff0000\"><path d=\"M 2.88 23.68 L 0 23.68 L 0 1.28 L 4.256 1.28 L 10.464 15.456 L 16.576 1.28 L 20.928 1.28 L 20.928 23.68 L 17.888 23.68 L 17.888 5.152 L 11.904 18.624 L 8.896 18.624 L 2.88 5.152 L 2.88 23.68 Z M 249.6 23.68 L 246.72 23.68 L 246.72 0 L 249.6 0 L 249.6 9.696 Q 250.496 8.384 251.872 7.52 A 5.623 5.623 0 0 1 254.152 6.717 A 7.23 7.23 0 0 1 255.104 6.656 A 7.432 7.432 0 0 1 256.725 6.822 Q 258.355 7.186 259.312 8.352 A 5.889 5.889 0 0 1 260.465 10.713 Q 260.704 11.679 260.704 12.832 L 260.704 23.68 L 257.824 23.68 L 257.824 13.152 A 6.28 6.28 0 0 0 257.717 11.955 Q 257.525 10.969 256.992 10.272 Q 256.16 9.184 254.656 9.184 A 4.233 4.233 0 0 0 252.508 9.781 A 5.627 5.627 0 0 0 251.856 10.224 Q 250.528 11.264 249.6 12.672 L 249.6 23.68 Z M 59.584 22.08 L 57.6 24 L 48.992 15.136 L 48.992 23.68 L 46.112 23.68 L 46.112 0 L 48.992 0 L 48.992 13.536 L 56.512 6.72 L 58.176 8.544 L 51.744 14.24 L 59.584 22.08 Z M 103.808 17.856 L 103.808 7.04 L 106.688 7.04 L 106.688 17.472 A 7.354 7.354 0 0 0 106.769 18.603 Q 106.861 19.194 107.058 19.67 A 3.244 3.244 0 0 0 107.52 20.464 A 2.689 2.689 0 0 0 109.156 21.431 A 4.098 4.098 0 0 0 109.952 21.504 Q 111.264 21.504 112.496 20.976 Q 113.728 20.448 114.688 19.552 L 114.688 7.04 L 117.568 7.04 L 117.568 19.36 A 4.992 4.992 0 0 0 117.601 19.953 Q 117.707 20.837 118.16 21.168 Q 118.752 21.6 119.456 21.6 L 118.848 23.84 Q 115.872 23.84 115.168 21.472 Q 114.048 22.624 112.576 23.344 Q 111.104 24.064 109.408 24.064 Q 107.968 24.064 106.688 23.456 A 4.621 4.621 0 0 1 104.925 21.972 A 5.845 5.845 0 0 1 104.608 21.488 A 5.197 5.197 0 0 1 104.075 20.178 Q 103.908 19.548 103.846 18.798 A 11.319 11.319 0 0 1 103.808 17.856 Z M 215.552 23.68 L 212.672 23.68 L 212.672 7.04 L 215.456 7.04 L 215.456 9.824 Q 216.352 8.48 217.744 7.568 Q 219.136 6.656 221.056 6.656 A 7.432 7.432 0 0 1 222.677 6.822 Q 224.307 7.186 225.264 8.352 A 5.889 5.889 0 0 1 226.417 10.713 Q 226.656 11.679 226.656 12.832 L 226.656 23.68 L 223.776 23.68 L 223.776 13.152 A 6.165 6.165 0 0 0 223.666 11.955 Q 223.471 10.969 222.928 10.272 A 2.745 2.745 0 0 0 220.919 9.199 A 3.86 3.86 0 0 0 220.576 9.184 Q 219.104 9.184 217.792 10.224 Q 216.48 11.264 215.552 12.672 L 215.552 23.68 Z M 87.552 22.336 L 88.768 19.936 A 5.938 5.938 0 0 0 89.774 20.616 Q 90.268 20.884 90.855 21.101 A 9.843 9.843 0 0 0 90.864 21.104 A 7.703 7.703 0 0 0 93.3 21.564 A 8.789 8.789 0 0 0 93.568 21.568 Q 95.294 21.568 96.203 21.019 A 2.476 2.476 0 0 0 96.272 20.976 A 2.384 2.384 0 0 0 96.763 20.562 A 1.617 1.617 0 0 0 97.184 19.456 A 2.279 2.279 0 0 0 97.084 18.769 A 1.988 1.988 0 0 0 96.848 18.272 A 2.043 2.043 0 0 0 96.551 17.922 Q 96.204 17.59 95.616 17.264 A 9.108 9.108 0 0 0 94.978 16.946 Q 94.301 16.638 93.344 16.303 A 33.156 33.156 0 0 0 93.024 16.192 A 14.917 14.917 0 0 1 91.617 15.634 Q 90.305 15.03 89.52 14.288 A 3.624 3.624 0 0 1 88.466 12.369 A 5.374 5.374 0 0 1 88.352 11.232 Q 88.352 9.312 90.032 7.984 A 5.685 5.685 0 0 1 91.964 7.009 Q 92.782 6.766 93.763 6.69 A 12.009 12.009 0 0 1 94.688 6.656 A 13.054 13.054 0 0 1 96.111 6.73 Q 96.873 6.814 97.536 6.992 A 12.406 12.406 0 0 1 98.859 7.426 A 9.955 9.955 0 0 1 99.776 7.84 L 98.976 10.208 A 7.849 7.849 0 0 0 97.802 9.643 A 9.661 9.661 0 0 0 97.008 9.376 A 8.008 8.008 0 0 0 95.573 9.105 A 10.188 10.188 0 0 0 94.56 9.056 Q 93.493 9.056 92.754 9.333 A 2.723 2.723 0 0 0 92.032 9.728 A 2.983 2.983 0 0 0 91.598 10.136 Q 91.168 10.636 91.168 11.2 Q 91.168 11.968 91.84 12.512 Q 92.512 13.056 94.464 13.696 A 21.681 21.681 0 0 1 96.084 14.303 Q 97.624 14.958 98.507 15.689 A 4.783 4.783 0 0 1 98.88 16.032 A 4.351 4.351 0 0 1 100.123 18.991 A 5.727 5.727 0 0 1 100.128 19.232 Q 100.128 21.472 98.32 22.768 A 6.417 6.417 0 0 1 96.279 23.719 Q 95.427 23.956 94.42 24.03 A 12.935 12.935 0 0 1 93.472 24.064 A 12.797 12.797 0 0 1 91.596 23.932 A 9.903 9.903 0 0 1 90.064 23.584 A 9.883 9.883 0 0 1 88.835 23.109 Q 88.11 22.768 87.552 22.336 Z M 281.824 22.336 L 283.04 19.936 A 5.938 5.938 0 0 0 284.046 20.616 Q 284.54 20.884 285.127 21.101 A 9.843 9.843 0 0 0 285.136 21.104 A 7.703 7.703 0 0 0 287.572 21.564 A 8.789 8.789 0 0 0 287.84 21.568 Q 289.566 21.568 290.475 21.019 A 2.476 2.476 0 0 0 290.544 20.976 A 2.384 2.384 0 0 0 291.035 20.562 A 1.617 1.617 0 0 0 291.456 19.456 A 2.279 2.279 0 0 0 291.356 18.769 A 1.988 1.988 0 0 0 291.12 18.272 A 2.043 2.043 0 0 0 290.823 17.922 Q 290.476 17.59 289.888 17.264 A 9.108 9.108 0 0 0 289.25 16.946 Q 288.573 16.638 287.616 16.303 A 33.156 33.156 0 0 0 287.296 16.192 A 14.917 14.917 0 0 1 285.889 15.634 Q 284.577 15.03 283.792 14.288 A 3.624 3.624 0 0 1 282.738 12.369 A 5.374 5.374 0 0 1 282.624 11.232 Q 282.624 9.312 284.304 7.984 A 5.685 5.685 0 0 1 286.236 7.009 Q 287.054 6.766 288.035 6.69 A 12.009 12.009 0 0 1 288.96 6.656 A 13.054 13.054 0 0 1 290.383 6.73 Q 291.145 6.814 291.808 6.992 A 12.406 12.406 0 0 1 293.131 7.426 A 9.955 9.955 0 0 1 294.048 7.84 L 293.248 10.208 A 7.849 7.849 0 0 0 292.074 9.643 A 9.661 9.661 0 0 0 291.28 9.376 A 8.008 8.008 0 0 0 289.845 9.105 A 10.188 10.188 0 0 0 288.832 9.056 Q 287.765 9.056 287.026 9.333 A 2.723 2.723 0 0 0 286.304 9.728 A 2.983 2.983 0 0 0 285.87 10.136 Q 285.44 10.636 285.44 11.2 Q 285.44 11.968 286.112 12.512 Q 286.784 13.056 288.736 13.696 A 21.681 21.681 0 0 1 290.356 14.303 Q 291.896 14.958 292.779 15.689 A 4.783 4.783 0 0 1 293.152 16.032 A 4.351 4.351 0 0 1 294.395 18.991 A 5.727 5.727 0 0 1 294.4 19.232 Q 294.4 21.472 292.592 22.768 A 6.417 6.417 0 0 1 290.551 23.719 Q 289.699 23.956 288.692 24.03 A 12.935 12.935 0 0 1 287.744 24.064 A 12.797 12.797 0 0 1 285.868 23.932 A 9.903 9.903 0 0 1 284.336 23.584 A 9.883 9.883 0 0 1 283.107 23.109 Q 282.382 22.768 281.824 22.336 Z M 164.8 22.304 L 164.8 23.68 L 162.112 23.68 L 162.112 0 L 164.992 0 L 164.992 8.768 A 4.964 4.964 0 0 1 165.596 8.189 Q 165.921 7.923 166.328 7.663 A 10.661 10.661 0 0 1 166.864 7.344 A 5.307 5.307 0 0 1 168.538 6.761 A 7.176 7.176 0 0 1 169.792 6.656 Q 171.712 6.656 173.392 7.664 A 7.145 7.145 0 0 1 175.689 9.893 A 8.789 8.789 0 0 1 176.096 10.576 Q 177.12 12.48 177.12 15.2 Q 177.12 17.184 176.48 18.816 A 9.397 9.397 0 0 1 175.468 20.737 A 8.154 8.154 0 0 1 174.768 21.616 Q 173.696 22.784 172.352 23.424 A 6.402 6.402 0 0 1 169.995 24.051 A 6.147 6.147 0 0 1 169.6 24.064 Q 168.16 24.064 166.896 23.536 A 9.688 9.688 0 0 1 165.839 23.021 Q 165.31 22.72 164.886 22.376 A 5.836 5.836 0 0 1 164.8 22.304 Z M 75.456 16.544 L 63.808 16.544 Q 64.076 18.72 65.261 20.008 A 4.794 4.794 0 0 0 65.44 20.192 A 4.526 4.526 0 0 0 67.88 21.411 A 6.407 6.407 0 0 0 68.992 21.504 Q 70.432 21.504 71.584 21.2 Q 72.736 20.896 73.792 20.416 L 74.464 22.88 A 13.046 13.046 0 0 1 73.077 23.4 A 16.596 16.596 0 0 1 71.872 23.728 A 12.453 12.453 0 0 1 70.243 23.99 A 16.648 16.648 0 0 1 68.64 24.064 A 9.062 9.062 0 0 1 66.161 23.742 A 6.755 6.755 0 0 1 62.96 21.776 A 7.527 7.527 0 0 1 61.294 18.723 Q 60.972 17.604 60.891 16.272 A 15.08 15.08 0 0 1 60.864 15.36 Q 60.864 12.864 61.808 10.896 A 7.885 7.885 0 0 1 63.413 8.63 A 7.324 7.324 0 0 1 64.448 7.792 A 6.735 6.735 0 0 1 67.731 6.681 A 8.386 8.386 0 0 1 68.384 6.656 Q 70.784 6.656 72.368 7.696 Q 73.952 8.736 74.752 10.496 Q 75.552 12.256 75.552 14.368 A 24.065 24.065 0 0 1 75.459 16.511 A 22.061 22.061 0 0 1 75.456 16.544 Z M 148.576 16.544 L 136.928 16.544 Q 137.196 18.72 138.381 20.008 A 4.794 4.794 0 0 0 138.56 20.192 A 4.526 4.526 0 0 0 141 21.411 A 6.407 6.407 0 0 0 142.112 21.504 Q 143.552 21.504 144.704 21.2 Q 145.856 20.896 146.912 20.416 L 147.584 22.88 A 13.046 13.046 0 0 1 146.197 23.4 A 16.596 16.596 0 0 1 144.992 23.728 A 12.453 12.453 0 0 1 143.363 23.99 A 16.648 16.648 0 0 1 141.76 24.064 A 9.062 9.062 0 0 1 139.281 23.742 A 6.755 6.755 0 0 1 136.08 21.776 A 7.527 7.527 0 0 1 134.414 18.723 Q 134.092 17.604 134.011 16.272 A 15.08 15.08 0 0 1 133.984 15.36 Q 133.984 12.864 134.928 10.896 A 7.885 7.885 0 0 1 136.533 8.63 A 7.324 7.324 0 0 1 137.568 7.792 A 6.735 6.735 0 0 1 140.851 6.681 A 8.386 8.386 0 0 1 141.504 6.656 Q 143.904 6.656 145.488 7.696 Q 147.072 8.736 147.872 10.496 Q 148.672 12.256 148.672 14.368 A 24.065 24.065 0 0 1 148.579 16.511 A 22.061 22.061 0 0 1 148.576 16.544 Z M 379.808 16.544 L 368.16 16.544 Q 368.428 18.72 369.613 20.008 A 4.794 4.794 0 0 0 369.792 20.192 A 4.526 4.526 0 0 0 372.232 21.411 A 6.407 6.407 0 0 0 373.344 21.504 Q 374.784 21.504 375.936 21.2 Q 377.088 20.896 378.144 20.416 L 378.816 22.88 A 13.046 13.046 0 0 1 377.429 23.4 A 16.596 16.596 0 0 1 376.224 23.728 A 12.453 12.453 0 0 1 374.595 23.99 A 16.648 16.648 0 0 1 372.992 24.064 A 9.062 9.062 0 0 1 370.513 23.742 A 6.755 6.755 0 0 1 367.312 21.776 A 7.527 7.527 0 0 1 365.646 18.723 Q 365.324 17.604 365.243 16.272 A 15.08 15.08 0 0 1 365.216 15.36 Q 365.216 12.864 366.16 10.896 A 7.885 7.885 0 0 1 367.765 8.63 A 7.324 7.324 0 0 1 368.8 7.792 A 6.735 6.735 0 0 1 372.083 6.681 A 8.386 8.386 0 0 1 372.736 6.656 Q 375.136 6.656 376.72 7.696 Q 378.304 8.736 379.104 10.496 Q 379.904 12.256 379.904 14.368 A 24.065 24.065 0 0 1 379.811 16.511 A 22.061 22.061 0 0 1 379.808 16.544 Z M 399.488 17.952 L 399.488 10.144 L 396.608 10.144 L 396.608 7.648 L 399.552 7.648 L 400.16 2.4 L 402.368 2.4 L 402.368 7.648 L 406.976 7.648 L 406.976 10.144 L 402.368 10.144 L 402.368 18.176 A 6.899 6.899 0 0 0 402.422 19.071 Q 402.573 20.225 403.152 20.768 Q 403.936 21.504 405.024 21.504 Q 405.824 21.504 406.544 21.248 Q 407.264 20.992 407.872 20.64 L 408.672 22.944 Q 408.032 23.328 406.944 23.696 Q 405.856 24.064 404.672 24.064 A 5.865 5.865 0 0 1 403.014 23.841 A 4.459 4.459 0 0 1 400.88 22.48 A 5.157 5.157 0 0 1 399.847 20.591 Q 399.603 19.814 399.525 18.874 A 11.129 11.129 0 0 1 399.488 17.952 Z M 243.104 7.648 L 242.24 10.08 Q 241.344 9.632 240.528 9.424 A 6.574 6.574 0 0 0 239.554 9.256 A 8.434 8.434 0 0 0 238.72 9.216 A 5.085 5.085 0 0 0 236.399 9.742 A 5.162 5.162 0 0 0 234.96 10.832 A 5.355 5.355 0 0 0 233.691 13.185 Q 233.471 14.02 233.444 15.01 A 10.328 10.328 0 0 0 233.44 15.296 Q 233.44 17.024 234.08 18.432 A 5.94 5.94 0 0 0 235.011 19.897 A 5.208 5.208 0 0 0 235.856 20.672 Q 236.992 21.504 238.528 21.504 Q 239.744 21.504 240.608 21.232 Q 241.469 20.961 242.456 20.452 A 16.647 16.647 0 0 0 242.464 20.448 L 243.36 22.88 A 9.132 9.132 0 0 1 241.883 23.522 A 10.973 10.973 0 0 1 241.136 23.744 Q 239.923 24.059 238.385 24.064 A 15.314 15.314 0 0 1 238.336 24.064 A 8.577 8.577 0 0 1 236.084 23.779 A 7.064 7.064 0 0 1 234.192 22.96 Q 232.416 21.856 231.456 19.888 A 9.396 9.396 0 0 1 230.612 17.068 A 12.167 12.167 0 0 1 230.496 15.36 A 10.252 10.252 0 0 1 230.916 12.389 A 9.215 9.215 0 0 1 231.472 10.976 Q 232.448 8.992 234.272 7.824 A 7.382 7.382 0 0 1 237.366 6.729 A 9.475 9.475 0 0 1 238.56 6.656 Q 239.844 6.656 240.849 6.869 A 7.67 7.67 0 0 1 241.04 6.912 Q 242.112 7.168 243.104 7.648 Z M 319.328 7.648 L 318.464 10.08 Q 317.568 9.632 316.752 9.424 A 6.574 6.574 0 0 0 315.778 9.256 A 8.434 8.434 0 0 0 314.944 9.216 A 5.085 5.085 0 0 0 312.623 9.742 A 5.162 5.162 0 0 0 311.184 10.832 A 5.355 5.355 0 0 0 309.915 13.185 Q 309.695 14.02 309.668 15.01 A 10.328 10.328 0 0 0 309.664 15.296 Q 309.664 17.024 310.304 18.432 A 5.94 5.94 0 0 0 311.235 19.897 A 5.208 5.208 0 0 0 312.08 20.672 Q 313.216 21.504 314.752 21.504 Q 315.968 21.504 316.832 21.232 Q 317.693 20.961 318.68 20.452 A 16.647 16.647 0 0 0 318.688 20.448 L 319.584 22.88 A 9.132 9.132 0 0 1 318.107 23.522 A 10.973 10.973 0 0 1 317.36 23.744 Q 316.147 24.059 314.609 24.064 A 15.314 15.314 0 0 1 314.56 24.064 A 8.577 8.577 0 0 1 312.308 23.779 A 7.064 7.064 0 0 1 310.416 22.96 Q 308.64 21.856 307.68 19.888 A 9.396 9.396 0 0 1 306.836 17.068 A 12.167 12.167 0 0 1 306.72 15.36 A 10.252 10.252 0 0 1 307.14 12.389 A 9.215 9.215 0 0 1 307.696 10.976 Q 308.672 8.992 310.496 7.824 A 7.382 7.382 0 0 1 313.59 6.729 A 9.475 9.475 0 0 1 314.784 6.656 Q 316.068 6.656 317.073 6.869 A 7.67 7.67 0 0 1 317.264 6.912 Q 318.336 7.168 319.328 7.648 Z M 395.232 7.648 L 394.368 10.08 Q 393.472 9.632 392.656 9.424 A 6.574 6.574 0 0 0 391.682 9.256 A 8.434 8.434 0 0 0 390.848 9.216 A 5.085 5.085 0 0 0 388.527 9.742 A 5.162 5.162 0 0 0 387.088 10.832 A 5.355 5.355 0 0 0 385.819 13.185 Q 385.599 14.02 385.572 15.01 A 10.328 10.328 0 0 0 385.568 15.296 Q 385.568 17.024 386.208 18.432 A 5.94 5.94 0 0 0 387.139 19.897 A 5.208 5.208 0 0 0 387.984 20.672 Q 389.12 21.504 390.656 21.504 Q 391.872 21.504 392.736 21.232 Q 393.597 20.961 394.584 20.452 A 16.647 16.647 0 0 0 394.592 20.448 L 395.488 22.88 A 9.132 9.132 0 0 1 394.011 23.522 A 10.973 10.973 0 0 1 393.264 23.744 Q 392.051 24.059 390.513 24.064 A 15.314 15.314 0 0 1 390.464 24.064 A 8.577 8.577 0 0 1 388.212 23.779 A 7.064 7.064 0 0 1 386.32 22.96 Q 384.544 21.856 383.584 19.888 A 9.396 9.396 0 0 1 382.74 17.068 A 12.167 12.167 0 0 1 382.624 15.36 A 10.252 10.252 0 0 1 383.044 12.389 A 9.215 9.215 0 0 1 383.6 10.976 Q 384.576 8.992 386.4 7.824 A 7.382 7.382 0 0 1 389.494 6.729 A 9.475 9.475 0 0 1 390.688 6.656 Q 391.972 6.656 392.977 6.869 A 7.67 7.67 0 0 1 393.168 6.912 Q 394.24 7.168 395.232 7.648 Z M 37.76 8.384 L 37.76 7.04 L 40.384 7.04 L 40.384 19.648 A 3.888 3.888 0 0 0 40.417 20.171 Q 40.499 20.774 40.788 21.079 A 0.949 0.949 0 0 0 40.976 21.232 Q 41.568 21.6 42.272 21.6 L 41.664 23.84 A 5.671 5.671 0 0 1 40.299 23.689 Q 38.496 23.241 37.985 21.466 A 4.349 4.349 0 0 1 37.952 21.344 A 6.818 6.818 0 0 1 36.957 22.44 A 9.119 9.119 0 0 1 35.952 23.216 Q 34.764 24.013 32.969 24.061 A 8.706 8.706 0 0 1 32.736 24.064 Q 30.656 24.064 28.96 23.008 A 7.343 7.343 0 0 1 26.588 20.6 A 8.935 8.935 0 0 1 26.256 20.016 A 8.713 8.713 0 0 1 25.414 17.474 A 11.726 11.726 0 0 1 25.248 15.456 A 10.605 10.605 0 0 1 25.567 12.81 A 8.95 8.95 0 0 1 26.256 10.976 Q 27.264 8.992 29.04 7.824 Q 30.816 6.656 33.12 6.656 A 7.227 7.227 0 0 1 34.654 6.813 A 5.985 5.985 0 0 1 35.728 7.152 A 8.321 8.321 0 0 1 37.1 7.886 A 7.073 7.073 0 0 1 37.76 8.384 Z M 204.32 8.384 L 204.32 7.04 L 206.944 7.04 L 206.944 19.648 A 3.888 3.888 0 0 0 206.977 20.171 Q 207.059 20.774 207.348 21.079 A 0.949 0.949 0 0 0 207.536 21.232 Q 208.128 21.6 208.832 21.6 L 208.224 23.84 A 5.671 5.671 0 0 1 206.859 23.689 Q 205.056 23.241 204.545 21.466 A 4.349 4.349 0 0 1 204.512 21.344 A 6.818 6.818 0 0 1 203.517 22.44 A 9.119 9.119 0 0 1 202.512 23.216 Q 201.324 24.013 199.529 24.061 A 8.706 8.706 0 0 1 199.296 24.064 Q 197.216 24.064 195.52 23.008 A 7.343 7.343 0 0 1 193.148 20.6 A 8.935 8.935 0 0 1 192.816 20.016 A 8.713 8.713 0 0 1 191.974 17.474 A 11.726 11.726 0 0 1 191.808 15.456 A 10.605 10.605 0 0 1 192.127 12.81 A 8.95 8.95 0 0 1 192.816 10.976 Q 193.824 8.992 195.6 7.824 Q 197.376 6.656 199.68 6.656 A 7.227 7.227 0 0 1 201.214 6.813 A 5.985 5.985 0 0 1 202.288 7.152 A 8.321 8.321 0 0 1 203.66 7.886 A 7.073 7.073 0 0 1 204.32 8.384 Z M 126.176 23.68 L 123.296 23.68 L 123.296 7.04 L 126.08 7.04 L 126.08 10.656 Q 126.496 9.6 127.2 8.688 Q 127.904 7.776 128.96 7.216 Q 130.016 6.656 131.424 6.656 Q 131.904 6.656 132.384 6.704 Q 132.864 6.752 133.184 6.848 L 132.32 9.824 A 3.124 3.124 0 0 0 131.633 9.628 Q 131.316 9.574 130.955 9.569 A 5.309 5.309 0 0 0 130.88 9.568 Q 129.696 9.568 128.624 10.24 Q 127.552 10.912 126.864 12.32 A 6.183 6.183 0 0 0 126.394 13.699 Q 126.246 14.366 126.198 15.145 A 12.509 12.509 0 0 0 126.176 15.904 L 126.176 23.68 Z M 184 23.68 L 181.12 23.68 L 181.12 7.04 L 183.904 7.04 L 183.904 10.656 Q 184.32 9.6 185.024 8.688 Q 185.728 7.776 186.784 7.216 Q 187.84 6.656 189.248 6.656 Q 189.728 6.656 190.208 6.704 Q 190.688 6.752 191.008 6.848 L 190.144 9.824 A 3.124 3.124 0 0 0 189.457 9.628 Q 189.14 9.574 188.779 9.569 A 5.309 5.309 0 0 0 188.704 9.568 Q 187.52 9.568 186.448 10.24 Q 185.376 10.912 184.688 12.32 A 6.183 6.183 0 0 0 184.218 13.699 Q 184.07 14.366 184.022 15.145 A 12.509 12.509 0 0 0 184 15.904 L 184 23.68 Z M 344.64 23.68 L 341.76 23.68 L 341.76 7.04 L 344.544 7.04 L 344.544 10.656 Q 344.96 9.6 345.664 8.688 Q 346.368 7.776 347.424 7.216 Q 348.48 6.656 349.888 6.656 Q 350.368 6.656 350.848 6.704 Q 351.328 6.752 351.648 6.848 L 350.784 9.824 A 3.124 3.124 0 0 0 350.097 9.628 Q 349.78 9.574 349.419 9.569 A 5.309 5.309 0 0 0 349.344 9.568 Q 348.16 9.568 347.088 10.24 Q 346.016 10.912 345.328 12.32 A 6.183 6.183 0 0 0 344.858 13.699 Q 344.71 14.366 344.662 15.145 A 12.509 12.509 0 0 0 344.64 15.904 L 344.64 23.68 Z M 357.408 23.68 L 354.528 23.68 L 354.528 7.04 L 357.312 7.04 L 357.312 10.656 Q 357.728 9.6 358.432 8.688 Q 359.136 7.776 360.192 7.216 Q 361.248 6.656 362.656 6.656 Q 363.136 6.656 363.616 6.704 Q 364.096 6.752 364.416 6.848 L 363.552 9.824 A 3.124 3.124 0 0 0 362.865 9.628 Q 362.548 9.574 362.187 9.569 A 5.309 5.309 0 0 0 362.112 9.568 Q 360.928 9.568 359.856 10.24 Q 358.784 10.912 358.096 12.32 A 6.183 6.183 0 0 0 357.626 13.699 Q 357.478 14.366 357.43 15.145 A 12.509 12.509 0 0 0 357.408 15.904 L 357.408 23.68 Z M 326.868 23.635 A 8.709 8.709 0 0 0 329.632 24.064 A 9.832 9.832 0 0 0 330.36 24.037 A 8.082 8.082 0 0 0 333.872 22.976 Q 335.744 21.888 336.832 19.92 A 8.402 8.402 0 0 0 337.681 17.635 A 10.939 10.939 0 0 0 337.92 15.296 A 11.053 11.053 0 0 0 337.765 13.405 A 8.254 8.254 0 0 0 336.816 10.704 Q 335.712 8.768 333.84 7.712 A 7.952 7.952 0 0 0 332.355 7.063 A 8.876 8.876 0 0 0 329.632 6.656 Q 327.296 6.656 325.408 7.712 Q 323.52 8.768 322.432 10.72 A 8.327 8.327 0 0 0 321.552 13.149 A 11.202 11.202 0 0 0 321.344 15.36 A 11.142 11.142 0 0 0 321.429 16.753 A 8.784 8.784 0 0 0 322.416 19.888 Q 323.488 21.856 325.376 22.96 A 7.859 7.859 0 0 0 326.868 23.635 Z M 278.144 23.68 L 275.264 23.68 L 275.264 7.04 L 278.144 7.04 L 278.144 23.68 Z M 415.168 17.056 L 412.8 17.056 L 412.384 8.32 L 412.384 0.352 L 415.584 0.352 L 415.584 8.32 L 415.168 17.056 Z M 329.632 21.504 A 5.997 5.997 0 0 0 331.304 21.283 A 4.48 4.48 0 0 0 333.568 19.792 Q 334.976 18.08 334.976 15.36 A 6.84 6.84 0 0 0 334.584 13.028 A 6.404 6.404 0 0 0 334.272 12.304 Q 333.568 10.912 332.368 10.064 Q 331.168 9.216 329.632 9.216 A 6.16 6.16 0 0 0 327.96 9.431 A 4.492 4.492 0 0 0 325.696 10.88 A 5.721 5.721 0 0 0 324.52 13.24 Q 324.31 14.09 324.29 15.088 A 10.518 10.518 0 0 0 324.288 15.296 Q 324.288 16.96 324.992 18.384 A 6.22 6.22 0 0 0 326.122 19.997 A 5.641 5.641 0 0 0 326.896 20.656 Q 328.096 21.504 329.632 21.504 Z M 37.504 19.424 L 37.504 10.496 Q 36.64 9.888 35.6 9.52 Q 34.56 9.152 33.408 9.152 A 5.096 5.096 0 0 0 31.817 9.393 A 4.488 4.488 0 0 0 30.704 9.936 A 5.216 5.216 0 0 0 29.098 11.672 A 6.388 6.388 0 0 0 28.864 12.112 A 6.612 6.612 0 0 0 28.303 13.929 A 8.857 8.857 0 0 0 28.192 15.36 Q 28.192 17.152 28.864 18.544 A 5.633 5.633 0 0 0 29.858 19.999 A 5.023 5.023 0 0 0 30.72 20.72 Q 31.904 21.504 33.376 21.504 Q 34.592 21.504 35.68 20.912 Q 36.768 20.32 37.504 19.424 Z M 204.064 19.424 L 204.064 10.496 Q 203.2 9.888 202.16 9.52 Q 201.12 9.152 199.968 9.152 A 5.096 5.096 0 0 0 198.377 9.393 A 4.488 4.488 0 0 0 197.264 9.936 A 5.216 5.216 0 0 0 195.658 11.672 A 6.388 6.388 0 0 0 195.424 12.112 A 6.612 6.612 0 0 0 194.863 13.929 A 8.857 8.857 0 0 0 194.752 15.36 Q 194.752 17.152 195.424 18.544 A 5.633 5.633 0 0 0 196.418 19.999 A 5.023 5.023 0 0 0 197.28 20.72 Q 198.464 21.504 199.936 21.504 Q 201.152 21.504 202.24 20.912 Q 203.328 20.32 204.064 19.424 Z M 164.992 11.328 L 164.992 20.224 Q 165.728 20.8 166.72 21.152 Q 167.712 21.504 168.832 21.504 A 5.437 5.437 0 0 0 170.437 21.274 A 4.719 4.719 0 0 0 171.6 20.736 A 5.104 5.104 0 0 0 173.248 19.005 A 6.287 6.287 0 0 0 173.488 18.56 A 6.678 6.678 0 0 0 174.079 16.625 A 8.793 8.793 0 0 0 174.176 15.296 Q 174.176 13.444 173.536 12.099 A 5.547 5.547 0 0 0 173.488 12 Q 172.8 10.624 171.664 9.888 Q 170.528 9.152 169.152 9.152 A 4.717 4.717 0 0 0 167.351 9.492 A 4.497 4.497 0 0 0 166.672 9.84 A 7.537 7.537 0 0 0 165.862 10.417 Q 165.466 10.744 165.175 11.093 A 4.141 4.141 0 0 0 164.992 11.328 Z M 63.776 14.208 L 72.896 14.208 Q 72.896 12.442 72.255 11.266 A 3.975 3.975 0 0 0 71.68 10.464 A 3.996 3.996 0 0 0 69.453 9.256 A 5.886 5.886 0 0 0 68.32 9.152 A 4.346 4.346 0 0 0 66.684 9.451 A 4.008 4.008 0 0 0 65.248 10.448 A 4.853 4.853 0 0 0 64.307 11.949 Q 64.037 12.608 63.889 13.415 A 10.247 10.247 0 0 0 63.776 14.208 Z M 136.896 14.208 L 146.016 14.208 Q 146.016 12.442 145.375 11.266 A 3.975 3.975 0 0 0 144.8 10.464 A 3.996 3.996 0 0 0 142.573 9.256 A 5.886 5.886 0 0 0 141.44 9.152 A 4.346 4.346 0 0 0 139.804 9.451 A 4.008 4.008 0 0 0 138.368 10.448 A 4.853 4.853 0 0 0 137.427 11.949 Q 137.157 12.608 137.009 13.415 A 10.247 10.247 0 0 0 136.896 14.208 Z M 368.128 14.208 L 377.248 14.208 Q 377.248 12.442 376.607 11.266 A 3.975 3.975 0 0 0 376.032 10.464 A 3.996 3.996 0 0 0 373.805 9.256 A 5.886 5.886 0 0 0 372.672 9.152 A 4.346 4.346 0 0 0 371.036 9.451 A 4.008 4.008 0 0 0 369.6 10.448 A 4.853 4.853 0 0 0 368.659 11.949 Q 368.389 12.608 368.241 13.415 A 10.247 10.247 0 0 0 368.128 14.208 Z M 412.532 23.444 A 2.054 2.054 0 0 0 414.016 24.064 A 2.754 2.754 0 0 0 414.215 24.057 A 2.067 2.067 0 0 0 415.552 23.472 A 1.93 1.93 0 0 0 416.062 22.633 A 2.462 2.462 0 0 0 416.16 21.92 A 1.992 1.992 0 0 0 415.975 21.065 A 2.226 2.226 0 0 0 415.504 20.416 A 2.713 2.713 0 0 0 415.337 20.266 A 2.024 2.024 0 0 0 414.016 19.776 A 2.82 2.82 0 0 0 413.423 19.835 A 1.913 1.913 0 0 0 412.448 20.352 A 1.897 1.897 0 0 0 411.946 21.261 A 2.76 2.76 0 0 0 411.872 21.92 Q 411.872 22.784 412.512 23.424 A 2.658 2.658 0 0 0 412.532 23.444 Z M 275.308 3.612 A 1.911 1.911 0 0 0 276.704 4.192 A 2.616 2.616 0 0 0 277.183 4.15 A 1.807 1.807 0 0 0 278.176 3.632 Q 278.72 3.072 278.72 2.176 A 2.325 2.325 0 0 0 278.72 2.15 A 1.939 1.939 0 0 0 278.128 0.752 A 2.366 2.366 0 0 0 278.109 0.734 A 1.939 1.939 0 0 0 276.704 0.16 A 2.644 2.644 0 0 0 276.174 0.211 A 1.812 1.812 0 0 0 275.232 0.704 A 1.788 1.788 0 0 0 274.761 1.546 A 2.556 2.556 0 0 0 274.688 2.176 A 1.917 1.917 0 0 0 274.845 2.95 A 2.126 2.126 0 0 0 275.28 3.584 A 2.391 2.391 0 0 0 275.308 3.612 Z\" vector-effect=\"non-scaling-stroke\"/></g></svg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "main-button"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/azakatov/ut-ml-project-2024/blob/main/main.ipynb\" target=\"_parent\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"117\" height=\"20\"><linearGradient id=\"b\" x2=\"0\" y2=\"100%\"><stop offset=\"0\" stop-color=\"#bbb\" stop-opacity=\".1\"/><stop offset=\"1\" stop-opacity=\".1\"/></linearGradient><clipPath id=\"a\"><rect width=\"117\" height=\"20\" rx=\"3\" fill=\"#fff\"/></clipPath><g clip-path=\"url(#a)\"><path fill=\"#555\" d=\"M0 0h30v20H0z\"/><path fill=\"#007ec6\" d=\"M30 0h87v20H30z\"/><path fill=\"url(#b)\" d=\"M0 0h117v20H0z\"/></g><g fill=\"#fff\" text-anchor=\"middle\" font-family=\"DejaVu Sans,Verdana,Geneva,sans-serif\" font-size=\"110\"><svg x=\"4px\" y=\"0px\" width=\"22px\" height=\"20px\" viewBox=\"-2 0 28 24\" style=\"background-color: #fff;border-radius: 1px;\"><path style=\"fill:#e8710a;\" d=\"M1.977,16.77c-2.667-2.277-2.605-7.079,0-9.357C2.919,8.057,3.522,9.075,4.49,9.691c-1.152,1.6-1.146,3.201-0.004,4.803C3.522,15.111,2.918,16.126,1.977,16.77z\"/><path style=\"fill:#f9ab00;\" d=\"M12.257,17.114c-1.767-1.633-2.485-3.658-2.118-6.02c0.451-2.91,2.139-4.893,4.946-5.678c2.565-0.718,4.964-0.217,6.878,1.819c-0.884,0.743-1.707,1.547-2.434,2.446C18.488,8.827,17.319,8.435,16,8.856c-2.404,0.767-3.046,3.241-1.494,5.644c-0.241,0.275-0.493,0.541-0.721,0.826C13.295,15.939,12.511,16.3,12.257,17.114z\"/><path style=\"fill:#e8710a;\" d=\"M19.529,9.682c0.727-0.899,1.55-1.703,2.434-2.446c2.703,2.783,2.701,7.031-0.005,9.764c-2.648,2.674-6.936,2.725-9.701,0.115c0.254-0.814,1.038-1.175,1.528-1.788c0.228-0.285,0.48-0.552,0.721-0.826c1.053,0.916,2.254,1.268,3.6,0.83C20.502,14.551,21.151,11.927,19.529,9.682z\"/><path style=\"fill:#f9ab00;\" d=\"M4.49,9.691C3.522,9.075,2.919,8.057,1.977,7.413c2.209-2.398,5.721-2.942,8.476-1.355c0.555,0.32,0.719,0.606,0.285,1.128c-0.157,0.188-0.258,0.422-0.391,0.631c-0.299,0.47-0.509,1.067-0.929,1.371C8.933,9.539,8.523,8.847,8.021,8.746C6.673,8.475,5.509,8.787,4.49,9.691z\"/><path style=\"fill:#f9ab00;\" d=\"M1.977,16.77c0.941-0.644,1.545-1.659,2.509-2.277c1.373,1.152,2.85,1.433,4.45,0.499c0.332-0.194,0.503-0.088,0.673,0.19c0.386,0.635,0.753,1.285,1.181,1.89c0.34,0.48,0.222,0.715-0.253,1.006C7.84,19.73,4.205,19.188,1.977,16.77z\"/></svg><text x=\"245\" y=\"140\" transform=\"scale(.1)\" textLength=\"30\"> </text><text x=\"725\" y=\"150\" fill=\"#010101\" fill-opacity=\".3\" transform=\"scale(.1)\" textLength=\"770\">Open in Colab</text><text x=\"725\" y=\"140\" transform=\"scale(.1)\" textLength=\"770\">Colab (main)</text></g> </svg></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqOO1ODOD8Dh"
   },
   "source": [
    "# Classification with an academic success dataset\n",
    "\n",
    "* Project members: Andres Alumets, Anton Zakatov, Pihla Järv, Muhammad Sohaib Anwar\n",
    "* Github: https://github.com/azakatov/ut-ml-project-2024\n",
    "* Kaggle: https://www.kaggle.com/competitions/playground-series-s4e6/overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b15UMowQthfN"
   },
   "source": [
    "Purpose of the project: contribute to the reduction of academic dropout and failure in higher education, by using machine learning techniques to identify students at risk at an early stage of their academic path, so that strategies to support them can be put into place <a href=\"https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success\">[1]</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pAGJ0RZGRGm"
   },
   "source": [
    "For this project we follow a workflow proposed by Will Koehrsen <a href=\"https://github.com/WillKoehrsen/machine-learning-project-walkthrough/blob/master/Machine%20Learning%20Project%20Part%201.ipynb\">[2]</a>.\n",
    "\n",
    "Applying their adjusted workflow, our report has the following structure:\n",
    "\n",
    "1. [Setup](#setup)\n",
    "1. [Data cleaning and formatting](#data-cleaning-and-formatting)\n",
    "1. [Exploratory data analysis](#exploratory-data-analysis)\n",
    "1. [Baseline model establishing](#baseline-model-establishing)\n",
    "1. [Models comparison](#models-comparison)\n",
    "1. [Feature engineering and selection](#feature-engineering-and-selection)\n",
    "1. [Hyperparameter tuning](#hyperparameter-tuning)\n",
    "1. [Model evaluation](#model-evaluation)\n",
    "1. [Results interpretation](#results-interpretation)\n",
    "1. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvglYGYbUcR7"
   },
   "source": [
    "<a name=\"setup\"></a>\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijv5xycyXC5e"
   },
   "source": [
    "### Common imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PaXcoJipiunH",
    "outputId": "eb7c2f9d-df9e-47f2-f385-4c282b385b89"
   },
   "outputs": [],
   "source": [
    "!pip install autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbpesFP9Ueyt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Show all columns in df\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dncVCbUcXH0m"
   },
   "source": [
    "### Setting up Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "GslKeb7ZWJSt",
    "outputId": "7de6ac43-d63d-4fc0-973d-ef5fd27213b6"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "# Upload your Kaggle token to use its API\n",
    "files.upload();\n",
    "\n",
    "!mkdir /root/.kaggle/\n",
    "!mv kaggle.json /root/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Seems to work without this line but breaks data loading if present. Commenting it out\n",
    "#!kaggle config set -n path -v{/content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynR5l90LWMe4",
    "outputId": "117eceea-6b9c-4059-f688-106e602b0117"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "!kaggle competitions download -c playground-series-s4e6\n",
    "!unzip playground-series-s4e6.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_0ygAz-E0Ib"
   },
   "source": [
    "<a name=\"data-cleaning-and-formatting\"></a>\n",
    "## Data cleaning and formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edtqX3NfeTQp"
   },
   "source": [
    "We start with data loading, cleaning and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFflTGGXD1tY"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
    "\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AsjbflregX6"
   },
   "source": [
    "Using the `head()` function we can get a better understanding regarding the structure of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "UBdfrA44X4NL",
    "outputId": "40188436-009d-436b-e156-b5012a87887e"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lpOugOufBac"
   },
   "source": [
    "From the output above we can see that all columns are of type `int` or `float`. Thus, the values themselves are not really interpretable. However, according to the competition's dataset description, the dataset was generated based on dataset available in <a href=\"https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success\"/>UC Irvine Machine Learning Repository</a>. In the repository there are also explanations regarding features' values. To have a better understanding about features and their values we provide these explanations in a table below.\n",
    "\n",
    "The table below has 7 columns:\n",
    "1. `Variable Name` - a name of a variable.\n",
    "2. `Role` - describes whether the variable is a feature or a target.\n",
    "3. `Type` - datatype of a varialbe (integer / continuous / categorical)\n",
    "4. `Demographic` - this column describes whether the column describe demographic information. Empty if it does not, contains a name of a demographic characteristic if it does.\n",
    "5. `Description` - describes what values of variables actually mean or how they mapped to the actual value.\n",
    "6. `Units` - unused.\n",
    "7. `Missing values` - represents whether there are any missing values in a column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pds2XdthiJpn"
   },
   "source": [
    "<table class=\"table my-4 w-full\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Variable Name</th>\n",
    "      <th>Role</th>\n",
    "      <th>Type</th>\n",
    "      <th>Demographic</th>\n",
    "      <th>Description</th>\n",
    "      <th>Units</th>\n",
    "      <th>Missing Values</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Marital Status</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td>Marital Status</td>\n",
    "      <td>1 – single<br />\n",
    "      2 – married <br />\n",
    "      3 – widower <br />\n",
    "      4 – divorced <br />\n",
    "      5 – facto union <br />\n",
    "      6 – legally separated\n",
    "      </td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Application mode</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>1 - 1st phase - general contingent<br />\n",
    "      2 - Ordinance No. 612/93<br />\n",
    "      5 - 1st phase - special contingent (Azores Island)<br />\n",
    "      7 - Holders of other higher courses <br />\n",
    "      10 - Ordinance No. 854-B/99 <br />\n",
    "      15 - International student (bachelor) <br />\n",
    "      16 - 1st phase - special contingent (Madeira Island) <br />\n",
    "      17 - 2nd phase - general contingent <br />\n",
    "      18 - 3rd phase - general contingent <br />\n",
    "      26 - Ordinance No. 533-A/99, item b2 (Different Plan) <br />\n",
    "      27 - Ordinance No. 533-A/99, item b3 (Other Institution) <br />\n",
    "      39 - Over 23 years old <br />\n",
    "      42 - Transfer <br />\n",
    "      43 - Change of course <br />\n",
    "      44 - Technological specialization diploma holders <br />\n",
    "      51 - Change of institution/course <br />\n",
    "      53 - Short cycle diploma holders <br />\n",
    "      57 - Change of institution/course (International)\n",
    "      </td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Application order</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Application order (between 0 - first choice; and 9 last choice)</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Course</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>33 - Biofuel Production Technologies <br />\n",
    "      171 - Animation and Multimedia Design <br />\n",
    "      8014 - Social Service (evening attendance) <br />\n",
    "      9003 - Agronomy <br />\n",
    "      9070 - Communication Design <br />\n",
    "      9085 - Veterinary Nursing <br />\n",
    "      9119 - Informatics Engineering <br />\n",
    "      9130 - Equinculture <br />\n",
    "      9147 - Management <br />\n",
    "      9238 - Social Service <br />\n",
    "      9254 - Tourism <br />\n",
    "      9500 - Nursing <br />\n",
    "      9556 - Oral Hygiene <br />\n",
    "      9670 - Advertising and Marketing Management <br />\n",
    "      9773 - Journalism and Communication <br />\n",
    "      9853 - Basic Education <br />\n",
    "      9991 - Management (evening attendance)\n",
    "      </td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Daytime/evening attendance</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>1 – daytime <br/> 0 - evening</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Previous qualification</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td>Education Level</td>\n",
    "      <td>1 - Secondary education <br/>\n",
    "      2 - Higher education - bachelor's degree <br/>\n",
    "      3 - Higher education - degree <br/>\n",
    "      4 - Higher education - master's <br/>\n",
    "      5 - Higher education - doctorate <br/>\n",
    "      6 - Frequency of higher education <br/>\n",
    "      9 - 12th year of schooling - not completed <br/>\n",
    "      10 - 11th year of schooling - not completed <br/>\n",
    "      12 - Other - 11th year of schooling <br/>\n",
    "      14 - 10th year of schooling <br/>\n",
    "      15 - 10th year of schooling - not completed <br/>\n",
    "      19 - Basic education 3rd cycle (9th/10th/11th year) or equiv. <br/>\n",
    "      38 - Basic education 2nd cycle (6th/7th/8th year) or equiv. <br/>\n",
    "      39 - Technological specialization course <br/>\n",
    "      40 - Higher education - degree (1st cycle) <br/>\n",
    "      42 - Professional higher technical course <br/>\n",
    "      43 - Higher education - master (2nd cycle)\n",
    "      </td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Previous qualification (grade)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Continuous</td>\n",
    "      <td></td>\n",
    "      <td>Grade of previous qualification (between 0 and 200)</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Nacionality</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td>Nationality</td>\n",
    "      <td>1 - Portuguese<br/>\n",
    "      2 - German<br/>\n",
    "      6 - Spanish<br/>\n",
    "      11 - Italian<br/>\n",
    "      13 - Dutch<br/>\n",
    "      14 - English<br/>\n",
    "      17 - Lithuanian<br/>\n",
    "      21 - Angolan<br/>\n",
    "      22 - Cape Verdean<br/>\n",
    "      24 - Guinean<br/>\n",
    "      25 - Mozambican<br/>\n",
    "      26 - Santomean<br/>\n",
    "      32 - Turkish<br/>\n",
    "      41 - Brazilian<br/>\n",
    "      62 - Romanian<br/>\n",
    "      100 - Moldova (Republic of)<br/>\n",
    "      101 - Mexican<br/>\n",
    "      103 - Ukrainian<br/>\n",
    "      105 - Russian<br/>\n",
    "      108 - Cuban<br/>\n",
    "      109 - Colombian\n",
    "      </td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Mother's qualification</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td>Education Level</td>\n",
    "      <td>1 - Secondary Education - 12th Year of Schooling or Eq. <br/>\n",
    "      2 - Higher Education - Bachelor's Degree <br/>\n",
    "      3 - Higher Education - Degree <br/>\n",
    "      4 - Higher Education - Master's <br/>\n",
    "      5 - Higher Education - Doctorate <br/>\n",
    "      6 - Frequency of Higher Education <br/>\n",
    "      9 - 12th Year of Schooling - Not Completed <br/>\n",
    "      10 - 11th Year of Schooling - Not Completed <br/>\n",
    "      11 - 7th Year (Old) <br/>\n",
    "      12 - Other - 11th Year of Schooling <br/>\n",
    "      14 - 10th Year of Schooling <br/>\n",
    "      18 - General commerce course <br/>\n",
    "      19 - Basic Education 3rd Cycle (9th/10th/11th Year) or Equiv. <br/>\n",
    "      22 - Technical-professional course <br/>\n",
    "      26 - 7th year of schooling <br/>\n",
    "      27 - 2nd cycle of the general high school course <br/>\n",
    "      29 - 9th Year of Schooling - Not Completed <br/>\n",
    "      30 - 8th year of schooling <br/>\n",
    "      34 - Unknown <br/>\n",
    "      35 - Can't read or write <br/>\n",
    "      36 - Can read without having a 4th year of schooling <br/>\n",
    "      37 - Basic education 1st cycle (4th/5th year) or equiv. <br/>\n",
    "      38 - Basic Education 2nd Cycle (6th/7th/8th Year) or Equiv. <br/>\n",
    "      39 - Technological specialization course <br/>\n",
    "      40 - Higher education - degree (1st cycle) <br/>\n",
    "      41 - Specialized higher studies course <br/>\n",
    "      42 - Professional higher technical course <br/>\n",
    "      43 - Higher Education - Master (2nd cycle) <br/>\n",
    "      44 - Higher Education - Doctorate (3rd cycle)\n",
    "      </td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Father's qualification</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td>Education Level</td>\n",
    "      <td>1 - Secondary Education - 12th Year of Schooling or Eq. <br/>\n",
    "      2 - Higher Education - Bachelor's Degree <br/>\n",
    "      3 - Higher Education - Degree <br/>\n",
    "      4 - Higher Education - Master's <br/>\n",
    "      5 - Higher Education - Doctorate <br/>\n",
    "      6 - Frequency of Higher Education <br/>\n",
    "      9 - 12th Year of Schooling - Not Completed <br/>\n",
    "      10 - 11th Year of Schooling - Not Completed <br/>\n",
    "      11 - 7th Year (Old) <br/>\n",
    "      12 - Other - 11th Year of Schooling <br/>\n",
    "      13 - 2nd year complementary high school course <br/>\n",
    "      14 - 10th Year of Schooling <br/>\n",
    "      18 - General commerce course <br/>\n",
    "      19 - Basic Education 3rd Cycle (9th/10th/11th Year) or Equiv. <br/>\n",
    "      20 - Complementary High School Course <br/>\n",
    "      22 - Technical-professional course <br/>\n",
    "      25 - Complementary High School Course - not concluded <br/>\n",
    "      26 - 7th year of schooling <br/>\n",
    "      27 - 2nd cycle of the general high school course <br/>\n",
    "      29 - 9th Year of Schooling - Not Completed <br/>\n",
    "      30 - 8th year of schooling <br/>\n",
    "      31 - General Course of Administration and Commerce <br/>\n",
    "      33 - Supplementary Accounting and Administration <br/>\n",
    "      34 - Unknown <br/>\n",
    "      35 - Can't read or write <br/>\n",
    "      36 - Can read without having a 4th year of schooling <br/>\n",
    "      37 - Basic education 1st cycle (4th/5th year) or equiv. <br/>\n",
    "      38 - Basic Education 2nd Cycle (6th/7th/8th Year) or Equiv. <br/>\n",
    "      39 - Technological specialization course <br/>\n",
    "      40 - Higher education - degree (1st cycle) <br/>\n",
    "      41 - Specialized higher studies course <br/>\n",
    "      42 - Professional higher technical course <br/>\n",
    "      43 - Higher Education - Master (2nd cycle) <br/>\n",
    "      44 - Higher Education - Doctorate (3rd cycle)</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Mother's occupation</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td>Occupation</td>\n",
    "      <td>0 - Student <br/>\n",
    "      1 - Representatives of the Legislative Power and Executive Bodies, Directors, Directors and Executive Managers <br/>\n",
    "      2 - Specialists in Intellectual and Scientific Activities <br/>\n",
    "      3 - Intermediate Level Technicians and Professions <br/>\n",
    "      4 - Administrative staff <br/>\n",
    "      5 - Personal Services, Security and Safety Workers and Sellers <br/>\n",
    "      6 - Farmers and Skilled Workers in Agriculture, Fisheries and Forestry <br/>\n",
    "      7 - Skilled Workers in Industry, Construction and Craftsmen <br/>\n",
    "      8 - Installation and Machine Operators and Assembly Workers <br/>\n",
    "      9 - Unskilled Workers <br/>\n",
    "      10 - Armed Forces Professions <br/>\n",
    "      90 - Other Situation <br/>\n",
    "      99 - (blank) <br/>\n",
    "      122 - Health professionals <br/>\n",
    "      123 - teachers <br/>\n",
    "      125 - Specialists in information and communication technologies (ICT) <br/>\n",
    "      131 - Intermediate level science and engineering technicians and professions <br/>\n",
    "      132 - Technicians and professionals, of intermediate level of health <br/>\n",
    "      134 - Intermediate level technicians from legal, social, sports, cultural and similar services <br/>\n",
    "      141 - Office workers, secretaries in general and data processing operators <br/>\n",
    "      143 - Data, accounting, statistical, financial services and registry-related operators <br/>\n",
    "      144 - Other administrative support staff <br/>\n",
    "      151 - personal service workers <br/>\n",
    "      152 - sellers <br/>\n",
    "      153 - Personal care workers and the like <br/>\n",
    "      171 - Skilled construction workers and the like, except electricians <br/>\n",
    "      173 - Skilled workers in printing, precision instrument manufacturing, jewelers, artisans and the like <br/>\n",
    "      175 - Workers in food processing, woodworking, clothing and other industries and crafts <br/>\n",
    "      191 - cleaning workers <br/>\n",
    "      192 - Unskilled workers in agriculture, animal production, fisheries and forestry <br/>\n",
    "      193 - Unskilled workers in extractive industry, construction, manufacturing and transport <br/>\n",
    "      194 - Meal preparation assistants</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Father's occupation</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td>Occupation</td>\n",
    "      <td>0 - Student <br/>\n",
    "      1 - Representatives of the Legislative Power and Executive Bodies, Directors, Directors and Executive Managers <br/>\n",
    "      2 - Specialists in Intellectual and Scientific Activities <br/>\n",
    "      3 - Intermediate Level Technicians and Professions <br/>\n",
    "      4 - Administrative staff <br/>\n",
    "      5 - Personal Services, Security and Safety Workers and Sellers <br/>\n",
    "      6 - Farmers and Skilled Workers in Agriculture, Fisheries and Forestry <br/>\n",
    "      7 - Skilled Workers in Industry, Construction and Craftsmen <br/>\n",
    "      8 - Installation and Machine Operators and Assembly Workers <br/>\n",
    "      9 - Unskilled Workers <br/>\n",
    "      10 - Armed Forces Professions <br/>\n",
    "      90 - Other Situation <br/>\n",
    "      99 - (blank) <br/>\n",
    "      101 - Armed Forces Officers <br/>\n",
    "      102 - Armed Forces Sergeants <br/>\n",
    "      103 - Other Armed Forces personnel <br/>\n",
    "      112 - Directors of administrative and commercial services <br/>\n",
    "      114 - Hotel, catering, trade and other services directors <br/>\n",
    "      121 - Specialists in the physical sciences, mathematics, engineering and related techniques <br/>\n",
    "      122 - Health professionals <br/>\n",
    "      123 - teachers <br/>\n",
    "      124 - Specialists in finance, accounting, administrative organization, public and commercial relations <br/>\n",
    "      131 - Intermediate level science and engineering technicians and professions <br/>\n",
    "      132 - Technicians and professionals, of intermediate level of health <br/>\n",
    "      134 - Intermediate level technicians from legal, social, sports, cultural and similar services <br/>\n",
    "      135 - Information and communication technology technicians <br/>\n",
    "      141 - Office workers, secretaries in general and data processing operators <br/>\n",
    "      143 - Data, accounting, statistical, financial services and registry-related operators <br/>\n",
    "      144 - Other administrative support staff <br/>\n",
    "      151 - personal service workers <br/>\n",
    "      152 - sellers <br/>\n",
    "      153 - Personal care workers and the like <br/>\n",
    "      154 - Protection and security services personnel <br/>\n",
    "      161 - Market-oriented farmers and skilled agricultural and animal production workers <br/>\n",
    "      163 - Farmers, livestock keepers, fishermen, hunters and gatherers, subsistence <br/>\n",
    "      171 - Skilled construction workers and the like, except electricians <br/>\n",
    "      172 - Skilled workers in metallurgy, metalworking and similar <br/>\n",
    "      174 - Skilled workers in electricity and electronics <br/>\n",
    "      175 - Workers in food processing, woodworking, clothing and other industries and crafts <br/>\n",
    "      181 - Fixed plant and machine operators <br/>\n",
    "      182 - assembly workers <br/>\n",
    "      183 - Vehicle drivers and mobile equipment operators <br/>\n",
    "      192 - Unskilled workers in agriculture, animal production, fisheries and forestry <br/>\n",
    "      193 - Unskilled workers in extractive industry, construction, manufacturing and transport <br/>\n",
    "      194 - Meal preparation assistants <br/>\n",
    "      195 - Street vendors (except food) and street service providers\n",
    "      </td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Admission grade</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Continuous</td>\n",
    "      <td></td>\n",
    "      <td>Admission grade (between 0 and 200)</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Displaced</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>1 – yes <br/>\n",
    "      0 – no</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Educational special needs</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>1 – yes <br/>\n",
    "      0 – no</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Debtor</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>1 – yes <br/>\n",
    "      0 – no</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Tuition fees up to date</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>1 – yes <br/>\n",
    "      0 – no</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Gender</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td>Gender</td>\n",
    "      <td>1 – male <br/>\n",
    "      0 – female</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Scholarship holder</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>1 – yes <br/>\n",
    "      0 – no</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Age at enrollment</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td>Age</td>\n",
    "      <td>Age of studend at enrollment</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>International</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>1 – yes <br/>\n",
    "      0 – no</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 1st sem (credited)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of curricular units credited in the 1st semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 1st sem (enrolled)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of curricular units enrolled in the 1st semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 1st sem (evaluations)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of evaluations to curricular units in the 1st semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 1st sem (approved)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of curricular units approved in the 1st semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 1st sem (grade)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Grade average in the 1st semester (between 0 and 20)</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 1st sem (without evaluations)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of curricular units without evalutions in the 1st semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 2nd sem (credited)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of curricular units credited in the 2nd semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 2nd sem (enrolled)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of curricular units enrolled in the 2nd semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 2nd sem (evaluations)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of evaluations to curricular units in the 2nd semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 2nd sem (approved)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of curricular units approved in the 2nd semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 2nd sem (grade)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Grade average in the 2nd semester (between 0 and 20)</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Curricular units 2nd sem (without evaluations)</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Integer</td>\n",
    "      <td></td>\n",
    "      <td>Number of curricular units without evalutions in the 1st semester</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Unemployment rate</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Continuous</td>\n",
    "      <td></td>\n",
    "      <td>Unemployment rate (%)</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Inflation rate</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Continuous</td>\n",
    "      <td></td>\n",
    "      <td>Inflation rate (%)</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>GDP</td>\n",
    "      <td>Feature</td>\n",
    "      <td>Continuous</td>\n",
    "      <td></td>\n",
    "      <td>GDP</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Target</td>\n",
    "      <td>Target</td>\n",
    "      <td>Categorical</td>\n",
    "      <td></td>\n",
    "      <td>Target. The problem is formulated as a three category classification task (dropout, enrolled, and graduate) at the end of the normal duration of the course</td>\n",
    "      <td></td>\n",
    "      <td>no</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCWstqYpeQdn"
   },
   "source": [
    "According to the table above, there are no missing values in any of the columns. Datatypes are also given in the table. As we do not use the original data but use the version generated by Kaggle, we validate that datatypes are actually the same and there are no missing values, using the function `info()`. Indeed, datatypes are `int` and `float` and there are no null values as there are 76518 entries and for each column the number of non-null values is also 76518."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Corr3pDTdfM3",
    "outputId": "f50cbd56-e14a-47ad-f99d-f122ed429cc4"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQfX2ZOgqyV5"
   },
   "source": [
    "Outliers detection with subsequent imputation plays an important role in training a machine learning model. If outliers are not treated properly, the quality of a trained model may be very low. For a starting point, it's worth using the `describe()` function in order to get an overview of a training set. Usually it is possible to see whether the values make any sense or not (e.g. negative distance between two cities), paying attention to statistics such as `max` or `min`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "SD_fW7h3dq3S",
    "outputId": "ba9f8d42-b92c-4df2-9173-668b35d53b85"
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUqLaFEyt-Ne"
   },
   "source": [
    "No outliers were detected in our analysis. However, several points are worth noting:\n",
    "1. The variable description table shows that many columns represent categorical codes, even though they use integer values. This makes outlier detection challenging, especially in the context of our project. For such columns, which are more categorical than numerical, the only feasible validation is to confirm that all values align with those defined in the variable descriptions. Outlier detection through unrealistic combinations of multiple features could be explored, but in our case, there are no clear contradictions between features (e.g., marital status is independent of nationality; a mother’s occupation is unrelated to a student's gender, etc.).\n",
    "2. Some columns in the training set, specifically numerical ones like `age at enrollment`, `unemployment rate`, `inflation rate`, `GDP`, `admission grade`, and `previous qualification (grade)`, could potentially contain outliers. We focused particularly on these features but found no outliers in this training set.\n",
    "3. A \"rigorous data preprocessing\" has already been conducted on the original data (from which the training set was generated) by the authors of the initial study <a url=\"https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success\">[1]</a>. This is most likely the reason why the given training set does not contain any outliers either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7pwkFGN6ErO"
   },
   "source": [
    "Two issues need to be addressed:\n",
    "1. The `id` column does not contain meaningful information for the model, so it can be removed.\n",
    "2. According to the variable description table, many columns represent categorical codes. Converting these values to strings is advisable so that ML algorithms do not treat them as numerical values with potential significance, avoiding using numerical semantics that lack true meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOsleFUs7VUD"
   },
   "outputs": [],
   "source": [
    "# Drop column 'id' in both train and test\n",
    "train.drop(\"id\", axis=1, inplace=True)\n",
    "test.drop(\"id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZEiVkdW8iAx",
    "outputId": "c771cdd8-a35c-4642-892a-6846595ea0db"
   },
   "outputs": [],
   "source": [
    "# Cast columns with categorical codes to strings\n",
    "numerical_features = [\n",
    "    \"Previous qualification (grade)\",\n",
    "    \"Admission grade\",\n",
    "    \"Age at enrollment\",\n",
    "    \"Curricular units 1st sem (credited)\",\n",
    "    \"Curricular units 1st sem (enrolled)\",\n",
    "    \"Curricular units 1st sem (evaluations)\",\n",
    "    \"Curricular units 1st sem (approved)\",\n",
    "    \"Curricular units 1st sem (grade)\",\n",
    "    \"Curricular units 1st sem (without evaluations)\",\n",
    "    \"Curricular units 2nd sem (credited)\",\n",
    "    \"Curricular units 2nd sem (enrolled)\",\n",
    "    \"Curricular units 2nd sem (evaluations)\",\n",
    "    \"Curricular units 2nd sem (approved)\",\n",
    "    \"Curricular units 2nd sem (grade)\",\n",
    "    \"Curricular units 2nd sem (without evaluations)\",\n",
    "    \"Unemployment rate\",\n",
    "    \"Inflation rate\",\n",
    "    \"GDP\"\n",
    "]\n",
    "\n",
    "categorical_features = list(train.columns)\n",
    "for col in numerical_features:\n",
    "    categorical_features.remove(col)\n",
    "categorical_features.remove(\"Target\")\n",
    "\n",
    "assert len(categorical_features) + len(numerical_features) + 1 == len(train.columns)\n",
    "\n",
    "for col in categorical_features:\n",
    "    train[col] = train[col].astype(str)\n",
    "    test[col] = test[col].astype(str)\n",
    "\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3850sxHIHwe"
   },
   "source": [
    "<a name=\"exploratory-data-analysis\"></a>\n",
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-kx10p96FV3"
   },
   "outputs": [],
   "source": [
    "# Imports for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXWlTAzF1cHF"
   },
   "source": [
    "Outliers detection with subsequent imputation plays an important role in training a machine learning model. If outliers are not treated properly, the quality of a trained model may be very low. For a starting point, it's worth using the `describe()` function in order to get an overview of a training set. Usually it is possible to see whether the values make any sense or not (e.g. negative distance between two cities), paying attention to statistics such as `max` or `min`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "Suroh61o1cHF",
    "outputId": "6cdef73e-e644-488f-f2ec-8d099d654809"
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zugU4mo1cHG"
   },
   "source": [
    "No outliers were detected in our analysis. However, several points are worth noting:\n",
    "1. The variable description table shows that many columns represent categorical codes, even though they use integer values. This makes outlier detection challenging, especially in the context of our project. For such columns, which are more categorical than numerical, the only feasible validation is to confirm that all values align with those defined in the variable descriptions. Outlier detection through unrealistic combinations of multiple features could be explored, but in our case, there are no clear contradictions between features (e.g., marital status is independent of nationality; a mother’s occupation is unrelated to a student's gender, etc.).\n",
    "2. Some columns in the training set, specifically numerical ones like `age at enrollment`, `unemployment rate`, `inflation rate`, `GDP`, `admission grade`, and `previous qualification (grade)`, could potentially contain outliers. We focused particularly on these features but found no outliers in this training set.\n",
    "3. A \"rigorous data preprocessing\" has already been conducted on the original data (from which the training set was generated) by the authors of the initial study <a url=\"https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success\">[1]</a>. This is most likely the reason why the given training set does not contain any outliers either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyyNtKi_3pwf"
   },
   "source": [
    "Next, we aim to visualize the relationships between specific features and target-class. For this, we chose such features as `Gender`, `Marital Status`, `Admission Grade`, `Age at Enrollment`, and some others. We hypothesize that these features might be the strongest predictors of the `Target`, though we acknowledge the possibility of bias in our assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "B1W2zqLa3wpk",
    "outputId": "ca868e64-8b37-44f9-b84c-342b3bcaf385"
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"Target\", hue=\"Gender\", data=train)\n",
    "plt.legend(labels = ['female', 'male'])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "ATj0RP89C2xK",
    "outputId": "534d95cd-445f-4aa8-851a-c278c603fa50"
   },
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"Target\", hue=\"Marital status\", data=train)\n",
    "#plt.legend(labels = ['female', 'male'])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "hDr9ofduDLSG",
    "outputId": "c727269e-7662-4dfe-a7be-5247f48b1eeb"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "colors = ['red', 'gold', 'green']\n",
    "\n",
    "for target, group in train.groupby('Target'):\n",
    "    sns.kdeplot(group[[\"Admission grade\"]].squeeze(), label=f'{target}', color=colors[idx])\n",
    "    idx += 1\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "abgBQiO5JRnE",
    "outputId": "97388331-0890-49bc-919f-04caed44f226"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "colors = ['red', 'gold', 'green']\n",
    "\n",
    "for target, group in train.groupby('Target'):\n",
    "    sns.kdeplot(group[[\"Age at enrollment\"]].squeeze(), label=f'{target}', color=colors[idx])\n",
    "    idx += 1\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "laZ0BozVMhU7",
    "outputId": "bc5212e9-1f97-4465-8ce6-1ea06413897c"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "colors = [\"red\", \"gold\", \"green\"]\n",
    "\n",
    "for target, group in train.groupby(\"Target\"):\n",
    "    sns.kdeplot(group[[\"Curricular units 1st sem (grade)\"]].squeeze(), label=f\"{target}\", color=colors[idx])\n",
    "    idx += 1\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "vLBxeuF-Mwwg",
    "outputId": "7c760881-1786-451b-823e-2217468e07e6"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "colors = [\"red\", \"gold\", \"green\"]\n",
    "\n",
    "for target, group in train.groupby(\"Target\"):\n",
    "    sns.kdeplot(group[[\"Curricular units 2nd sem (grade)\"]].squeeze(), label=f\"{target}\", color=colors[idx])\n",
    "    idx += 1\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di1J0HAs1htu"
   },
   "source": [
    "For a high-quality ML model, it is crucial to verify the balance of the training set. If the training data is imbalanced, the model may fail to adequately learn from or attend to underrepresented target classes, leading to biased predictions and reduced performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "id": "RlmZ6YUc6Si4",
    "outputId": "a3409699-7ae8-4ca5-eea7-f24667aa4a4b"
   },
   "outputs": [],
   "source": [
    "train[\"Target\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "eXJ9p_oF28bC",
    "outputId": "dbb192cd-64cf-4619-f7ff-64e5d9540535"
   },
   "outputs": [],
   "source": [
    "train[\"Target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "582TTR7w3nee"
   },
   "source": [
    "The output above shows that there are significantly more rows with the target \"Graduate\" compared to \"Dropout\" and \"Enrolled.\" Thus, \"Dropout\" and \"Enrolled\" are underrepresented, and we need to apply an oversampling technique. In the following cell, we use SMOTE to address this imbalance. The output confirms that the classes are now balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "VQ7FY4Ih3m1X",
    "outputId": "217c5453-0d0d-456d-d8fa-6bffc41907e6"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "smote = SMOTENC(random_state=42, categorical_features=categorical_features)\n",
    "X_train, y_train = smote.fit_resample(train.drop(\"Target\", axis=1), train[\"Target\"])\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "train[\"Target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJ5VXfrq0tZ2"
   },
   "outputs": [],
   "source": [
    "X_test = test # Rename it to be more stylish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrLvQgua1fPz"
   },
   "source": [
    "#### Visualizing distribution of values by target class for all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJgAo9Gr1kBD"
   },
   "outputs": [],
   "source": [
    "# Define the color palette and the fixed order for the target categories\n",
    "target_order = [\"Dropout\", \"Enrolled\", \"Graduate\"]\n",
    "colors = [\"red\", \"gold\", \"green\"]\n",
    "\n",
    "# Define line styles for the target categories\n",
    "line_styles = [\"solid\", \"dashed\", \"dotted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ge-jTwVR1lq9",
    "outputId": "268a3029-e0d5-404b-ce3f-84ddf19d92e0"
   },
   "outputs": [],
   "source": [
    "### Plots for Numerical Features\n",
    "\n",
    "max_plots_per_row = 3\n",
    "num_cols = len(numerical_features)\n",
    "num_rows = (num_cols + max_plots_per_row - 1) // max_plots_per_row  # Ensures enough rows for all plots\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, max_plots_per_row, figsize=(20, 6 * num_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# General title\n",
    "fig.suptitle(\"Density Distribution of Numerical Values by Target Class\", fontsize=20, y=1.0)\n",
    "\n",
    "\n",
    "for i, col in enumerate(numerical_features):\n",
    "    bins = train[col].max() - train[col].min() + 1\n",
    "\n",
    "    # Calculate bin edges if column is integer type\n",
    "    if pd.api.types.is_integer_dtype(train[col]):\n",
    "        min_value = train[col].min()\n",
    "        max_value = train[col].max()\n",
    "\n",
    "        # Define bins so that each integer value will have their own bin\n",
    "        bin_edges = np.linspace(min_value, max_value, max_value - min_value + 1)\n",
    "\n",
    "    for idx, target in enumerate(target_order):\n",
    "        # Filter the group based on the defined target order\n",
    "        group = train[train[\"Target\"] == target]\n",
    "\n",
    "        # Greate graph based on numberic type\n",
    "        if pd.api.types.is_integer_dtype(train[col]):\n",
    "            sns.histplot(data=group, x=col, label=target, color=colors[idx], ax=axes[i],\n",
    "                     kde=False, stat=\"density\", bins=bin_edges, element=\"step\", fill=False, linestyle=line_styles[idx % len(line_styles)])\n",
    "        elif pd.api.types.is_float_dtype(train[col]):\n",
    "            sns.kdeplot(group[col].squeeze(), label=target, color=colors[idx], ax=axes[i], linestyle=line_styles[idx % len(line_styles)])\n",
    "        else:\n",
    "            raise Exception(f\"{col} is not a numerical type\")\n",
    "\n",
    "\n",
    "    # Set titles and labels\n",
    "    axes[i].set_title(f\"Distribution of '{col}' by Target Class\")\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "    axes[i].legend(title=\"Target\", loc=\"upper right\")\n",
    "\n",
    "# Hide any unused subplots if num_cols is not a perfect multiple of max_plots_per_row\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ijgPAlt51ofR",
    "outputId": "513c10df-04d2-4b56-9a2f-5013fa79cad1"
   },
   "outputs": [],
   "source": [
    "### Plots for categorical features\n",
    "\n",
    "max_plots_per_row = 3\n",
    "num_cols = len(categorical_features)\n",
    "num_rows = (num_cols // max_plots_per_row) + (1 if num_cols % max_plots_per_row != 0 else 0)\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, max_plots_per_row, figsize=(20, 6 * num_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# General title\n",
    "fig.suptitle(\"Density Distribution of Categorical Values by Target Class\", fontsize=20, y=1.0)\n",
    "\n",
    "for i, col in enumerate(categorical_features):\n",
    "    # Get the value counts and calculate cumulative frequency\n",
    "    value_counts = train[col].value_counts(normalize=True)\n",
    "    cumulative_frequency = value_counts.cumsum()\n",
    "\n",
    "    # Determine total unique values\n",
    "    total_unique_values = train[col].nunique()\n",
    "\n",
    "    # Show only top max 10 values\n",
    "    num_values_to_show = min(10, total_unique_values)\n",
    "\n",
    "    # Get the top values based on the calculated number to display\n",
    "    top_values = value_counts.nlargest(num_values_to_show).index.tolist()\n",
    "    train_top = train[train[col].isin(top_values)]\n",
    "\n",
    "    # Calculate counts for each category within each target class\n",
    "    counts = train_top.groupby([\"Target\", col]).size().reset_index(name=\"Count\")\n",
    "\n",
    "    # Normalize counts to get proportions within each target group\n",
    "    total_counts = counts.groupby(\"Target\")[\"Count\"].transform(\"sum\")\n",
    "    counts[\"Proportion\"] = counts[\"Count\"] / total_counts\n",
    "\n",
    "    # Plot proportions\n",
    "    sns.barplot(\n",
    "        data=counts, x=col, y=\"Proportion\", hue=\"Target\", ax=axes[i],\n",
    "        palette=colors, hue_order=target_order, order=top_values\n",
    "    )\n",
    "\n",
    "    # Set titles and labels\n",
    "    axes[i].set_title(f\"Distribution of '{col}' by Target Class (top {num_values_to_show}/{total_unique_values})\")\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel(\"Density\")\n",
    "    axes[i].legend(title=\"Target\", loc=\"upper right\")\n",
    "\n",
    "# Hide any unused subplots if num_cols is not a perfect multiple of max_plots_per_row\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUwoxU1-IdIF"
   },
   "source": [
    "<a name=\"baseline-model-establishing\"></a>\n",
    "## Baseline model establishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vy1b_4G5MBrL"
   },
   "source": [
    "We established two baseline models. The first model predicts the target classes entirely at random, using the function `random_predict` (see below). With this function, we achieved a private score (accuracy) of 0.33501 and a public score of 0.33542."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyxF8Ex4ImNo"
   },
   "outputs": [],
   "source": [
    "test_first_id = 76518\n",
    "\n",
    "def random_predict(test):\n",
    "  np.random.seed(42)\n",
    "  return np.random.choice([\"Graduate\", \"Dropout\", \"Enrolled\"], len(test))\n",
    "\n",
    "random_predict_submission = pd.DataFrame({'id': list(range(test_first_id, test_first_id + len(test))),\n",
    "                                          'Target': random_predict(test)})\n",
    "random_predict_submission.to_csv('random_predict_submission.csv', index=False)\n",
    "\n",
    "!kaggle competitions submit -c playground-series-s4e6 -f random_predict_submission.csv -m \"Submission using random_predict\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSNtALrNMwt9"
   },
   "source": [
    "We also tested another baseline model that predicts the target classes randomly, but with the same distribution as the training data, accounting for the observed class imbalance. This model is implemented as the function `random_predict_with_dist` (see below). Using this function, we achieved a private score of 0.37522 and a public score of 0.36865."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xIw4zWRKQD6"
   },
   "outputs": [],
   "source": [
    "def random_predict_with_dist(test):\n",
    "  np.random.seed(42)\n",
    "  classes = [\"Graduate\", \"Dropout\", \"Enrolled\"]\n",
    "  num_per_class = [36282, 25296, 14940]\n",
    "  num_total = sum(num_per_class)\n",
    "  distribution = [num / num_total for num in num_per_class]\n",
    "  return np.random.choice(classes, len(test), p=distribution)\n",
    "\n",
    "random_predict_with_dist_submission = pd.DataFrame({'id': list(range(test_first_id, test_first_id + len(test))),\n",
    "                                                    'Target': random_predict_with_dist(test)})\n",
    "random_predict_with_dist_submission.to_csv('random_predict_with_dist_submission.csv', index=False)\n",
    "\n",
    "!kaggle competitions submit -c playground-series-s4e6 -f random_predict_with_dist_submission.csv -m \"Submission using random_predict_with_dist\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORrcxbL-QIKs"
   },
   "source": [
    "A more meaningful baseline model might be a standard Logistic Regression. Unlike random guessing, this model learns from the training data to make predictions. With this model, we achieved a private score of 0.76505 and a public score of 0.76220. While these scores may seem reasonable, they are actually quite low: in a competition, this performance would rank around 2497th out of 2685. For comparison, the first-place model has an accuracy of 0.84035."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHp6jNH6O0cr"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(train.drop(\"Target\", axis=1), train[\"Target\"])\n",
    "\n",
    "logistic_regression_submission = pd.DataFrame({'id': list(range(test_first_id, test_first_id + len(test))),\n",
    "                                                'Target': model.predict(test)})\n",
    "logistic_regression_submission.to_csv('logistic_regression_submission.csv', index=False)\n",
    "\n",
    "!kaggle competitions submit -c playground-series-s4e6 -f logistic_regression_submission.csv -m \"Submission using logistic regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7qFDOjjNv-G"
   },
   "source": [
    "In the following section, we will explore more sophisticated models with the goal of achieving significantly better performance than the baseline models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI0J4KBLIsGE"
   },
   "source": [
    "<a name=\"models-comparison\"></a>\n",
    "## Models comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNw543lk0tZ9"
   },
   "source": [
    "We aim to get improved results. To start, we try stacking using two models: Logistic Regression and Random Forest Classifier. For final estimator we use Support Vector Machine, namely Linear Support Vector Classification. In the next cell we fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "87iTkMCM0tZ9",
    "outputId": "f64407e4-db67-4f3b-91df-37fe55581f07"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimators = [\n",
    "    ('log_reg', LogisticRegression()),\n",
    "    ('random_forest', RandomForestClassifier())\n",
    "]\n",
    "\n",
    "np.random.seed(42)\n",
    "stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=LinearSVC(), cv=4)\n",
    "stacking_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YqVUD830tZ-"
   },
   "source": [
    "In the next cell we predict classes for test dataset and submit it to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "foWnPp-40tZ-"
   },
   "outputs": [],
   "source": [
    "y_pred = stacking_classifier.predict(X_test)\n",
    "test_first_id = 76518\n",
    "stacking_submission = pd.DataFrame({'id': list(range(test_first_id, test_first_id + len(test))),\n",
    "                                    'Target': y_pred})\n",
    "\n",
    "stacking_submission.to_csv('stacking_submission.csv', index=False)\n",
    "!kaggle competitions submit -c playground-series-s4e6 -f stacking_submission.csv -m \"Submission using stacking\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cw8la8x0tZ-"
   },
   "source": [
    "With this model, we achieved a private score of 0.82271 and a public score of 0.82474, which would place us at rank 2193. While this meets the minimum goal (according to our plan the minimum goal was at least 0.80000), there's still significant work ahead to reach the ultimate target (accuracy 0.83500 or higher)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6feYVy15Di6c"
   },
   "source": [
    "<img src=\"https://i.imgur.com/lgH4FdB.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQaQQJs3dL51"
   },
   "source": [
    "We also tried GradientBoostingClassifier. Unfortunately, this gave even worse accuracy of 0.82032."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zfbwZR8ETXs"
   },
   "source": [
    "<img src=\"https://i.imgur.com/cvKNamj.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "id": "CiLD9Uawdd4S",
    "outputId": "40211991-dce7-4dd8-87cb-905e1854a433"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7mHqAAsetLu",
    "outputId": "c5e482a5-29ee-4d23-8b3d-6e8e381373f5"
   },
   "outputs": [],
   "source": [
    "y_pred = gb_model.predict(X_test)\n",
    "test_first_id = 76518\n",
    "gb_submission = pd.DataFrame({'id': list(range(test_first_id, test_first_id + len(test))),\n",
    "                                    'Target': y_pred})\n",
    "\n",
    "gb_submission.to_csv('gb_submission.csv', index=False)\n",
    "!kaggle competitions submit -c playground-series-s4e6 -f gb_submission.csv -m \"Submission using GB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12GXwNY2jp9e"
   },
   "source": [
    "And we also tried AdaBoostClassifier. This game us accuracy of 0.81238, which is not an improvement :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19FIjB_dExgP"
   },
   "source": [
    "<img src=\"https://i.imgur.com/X9MJ6TU.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "MhYlpFUujoqn",
    "outputId": "3b525ba9-4b04-4701-f0fa-3bb9c687787d"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ab_model = AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "ab_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qm8xYBFj4hD",
    "outputId": "ba4cac22-387e-49b7-ace7-2dca25b6ed2f"
   },
   "outputs": [],
   "source": [
    "y_pred = ab_model.predict(X_test)\n",
    "test_first_id = 76518\n",
    "ab_submission = pd.DataFrame({'id': list(range(test_first_id, test_first_id + len(test))),\n",
    "                                    'Target': y_pred})\n",
    "\n",
    "ab_submission.to_csv('ab_submission.csv', index=False)\n",
    "!kaggle competitions submit -c playground-series-s4e6 -f ab_submission.csv -m \"Submission using AB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-odldELrkCsC"
   },
   "source": [
    "Of course, participating in a Kaggle competition means you should try XGBoost. We had high hopes for this model. But it gave almost the worst accuracy, being better than LogisticRegression only by ~1%. Namely, using XGB gave us public score of 0.79523."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2WgFi_SFlap"
   },
   "source": [
    "<img src=\"https://i.imgur.com/p4qWlV7.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t--wCOiRGAJh"
   },
   "source": [
    "TLDR:\n",
    "\n",
    "<img src=\"https://www.epmguidance.com/wp-content/uploads/2010/03/no-free-lunch.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "9PZXPlX8kFSn",
    "outputId": "e72061c5-9fe3-41e2-d527-0ea22bcc18a5"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "target_mapping = {'Dropout': 0, 'Enrolled': 1, 'Graduate': 2}\n",
    "y_train_xgb = train['Target'].map(target_mapping)\n",
    "X_train_xgb = train.drop('Target', axis=1)\n",
    "for col in X_train_xgb.select_dtypes(include=['object']).columns:\n",
    "    X_train_xgb[col] = X_train_xgb[col].astype('category')\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, learning_rate=0.01, random_state=42, enable_categorical=True, objective=\"multi:softmax\")\n",
    "xgb_model.fit(X_train_xgb, y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqOb4jVlkR85",
    "outputId": "92221227-0989-4d4f-da5f-66dce2d830c5"
   },
   "outputs": [],
   "source": [
    "X_test_xgb = X_test.copy()\n",
    "for col in X_test_xgb.select_dtypes(include=['object']).columns:\n",
    "    X_test_xgb[col] = X_test_xgb[col].astype('category')\n",
    "\n",
    "y_pred = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "target_mapping_reverse = {v: k for k, v in target_mapping.items()}\n",
    "y_pred = np.array([target_mapping_reverse[pred] for pred in y_pred])\n",
    "\n",
    "test_first_id = 76518\n",
    "xgb_submission = pd.DataFrame({'id': list(range(test_first_id, test_first_id + len(test))),\n",
    "                                    'Target': y_pred})\n",
    "\n",
    "xgb_submission.to_csv('xgb_submission.csv', index=False)\n",
    "!kaggle competitions submit -c playground-series-s4e6 -f xgb_submission.csv -m \"Submission using XGB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAkwUCcTqEbv"
   },
   "source": [
    "We had hopes that an ensemble of GradientBoostingClassifier, AdaBoostClassifier and XGBClassified could do some better work. However, an ensemble of these models was worse than GradientBoostingClassifier solely. Accuracy was 0.81954."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2aaeM3pG4IO"
   },
   "source": [
    "<img src=\"https://i.imgur.com/qubkruB.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "z8auqsF9qDzK",
    "outputId": "8b01c114-7b80-4855-f18b-8caf3eac68f3"
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    (\"gb\", GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
    "    (\"ab\", AdaBoostClassifier(n_estimators=50, learning_rate=1.0, random_state=42)),\n",
    "    (\"xgb\", xgb.XGBClassifier(n_estimators=100, learning_rate=0.01, random_state=42, enable_categorical=True, objective=\"multi:softmax\"))\n",
    "]\n",
    "\n",
    "np.random.seed(42)\n",
    "ensemble_classifier = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=4)\n",
    "ensemble_classifier.fit(X_train_xgb, y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-iFzhYlqzRY",
    "outputId": "73cb8325-b44c-41ac-8f0e-adb93656f1d6"
   },
   "outputs": [],
   "source": [
    "y_pred = ensemble_classifier.predict(X_test_xgb)\n",
    "y_pred = np.array([target_mapping_reverse[pred] for pred in y_pred])\n",
    "\n",
    "test_first_id = 76518\n",
    "ensemble_submission = pd.DataFrame({'id': list(range(test_first_id, test_first_id + len(test))),\n",
    "                                    'Target': y_pred})\n",
    "\n",
    "ensemble_submission.to_csv('ensemble_submission.csv', index=False)\n",
    "!kaggle competitions submit -c playground-series-s4e6 -f ensemble_submission.csv -m \"Submission using Ensemble\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJEtJKrZ1XGB"
   },
   "source": [
    "Finally, we decided to try out something fancy. Namely, we tried out such tool as AutoGluon (https://auto.gluon.ai/stable/index.html).\n",
    "> AutoGluon is an open-source machine learning library developed by Amazon that focuses on automating machine learning (AutoML) tasks. It is designed to simplify the process of building, training, and deploying machine learning models.\n",
    "\n",
    "Main features of it are:\n",
    "* Quick Prototyping - Build machine learning solutions on raw data in a few lines of code.\n",
    "* State-of-the-art Techniques - Automatically utilize SOTA models without expert knowledge.\n",
    "* Easy to Deploy - Move from experimentation to production with cloud predictors and pre-built containers.\n",
    "* Customizable - Extensible with custom feature processing, models, and metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o92bf8N5ISJ8"
   },
   "source": [
    "Training a model with Gluon took more time but eventually it gave some good results. We used Gluon with default settings (actually, we tried to use some of its parameters such as `presets`, `eval_metric` and others but this didn't improve the results) and this tool came up with a model that received accuracy of 0.83336."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5481sNewIzX_"
   },
   "source": [
    "<img src=\"https://i.imgur.com/jQwFtMs.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sr5MqOWi1YU5"
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "train_data = TabularDataset(train)\n",
    "test_data = TabularDataset(test)\n",
    "val_data = TabularDataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D__Z3tpl8uOS",
    "outputId": "8b228d0e-6bdd-4757-b3df-d0663a15686e"
   },
   "outputs": [],
   "source": [
    "predictor = TabularPredictor(label='Target').fit(train_data=train_data)\n",
    "predictions = predictor.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLAlqlQNq1Su",
    "outputId": "e7fa601d-c2c1-4717-d2ba-5900497ba70b"
   },
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "path = \"drive/MyDrive/AutogluonModels/ag-20241217_190525\"\n",
    "predictor = TabularPredictor.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E2iStiFw7GPG",
    "outputId": "d8b4b739-9408-4987-e08a-cfb0477d67f8"
   },
   "outputs": [],
   "source": [
    "test_first_id = 76518\n",
    "gluon_submission = pd.DataFrame({'id': list(range(test_first_id, test_first_id + len(test))),\n",
    "                                    'Target': predictions})\n",
    "\n",
    "gluon_submission.to_csv('gluon_submission.csv', index=False)\n",
    "!kaggle competitions submit -c playground-series-s4e6 -f gluon_submission.csv -m \"Submission using Gluon\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBahwf_5zxCP"
   },
   "source": [
    "Based on the output of a next cell, the best model was WeightedEnsemble_L2. Based on the `predictor.info()`, this is an ensemble model that consists of 4 models with the following weights:\n",
    "\n",
    "|index|model|weight|model type|used features|hyperparameters|\n",
    "|---|---|---|---|---|---|\n",
    "|1|'KNeighborsDist'|0.38095238095238093|KNN|'Previous qualification (grade)',  'Admission grade',  'Age at enrollment',  'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',  <br>'Curricular units 1st sem (evaluations)',  'Curricular units 1st sem (approved)',  'Curricular units 1st sem (grade)',  'Curricular units 1st sem (without evaluations)', <br>  'Curricular units 2nd sem (credited)',  'Curricular units 2nd sem (enrolled)',  'Curricular units 2nd sem (evaluations)',  'Curricular units 2nd sem (approved)', <br> 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',  'Unemployment rate',  'Inflation rate',  'GDP'|'weights': 'distance'|\n",
    "|2|'LightGBMXT'|0.047619047619047616|Light GBM with extra trees|All features|'learning_rate': 0.05, <br> 'extra_trees': True|\n",
    "|3|'XGBoost'|0.38095238095238093|XGBoost|All features|'n_estimators': 10000, <br> 'learning_rate': 0.1, <br> 'n_jobs': -1, <br> 'proc.max_category_levels': 100, <br> 'objective': 'multi:softprob', <br> 'booster': 'gbtree', <br> 'num_class': 3|\n",
    "|4|'LightGBMLarge'|0.19047619047619047|Light GBM |All features|'learning_rate': 0.03, <br> 'num_leaves': 128, <br> 'feature_fraction': 0.9, <br> 'min_data_in_leaf': 3|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bdqlreeouwif"
   },
   "outputs": [],
   "source": [
    "predictor.leaderboard(silent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jlfYa6HzQjW"
   },
   "source": [
    "|index|model|score\\_val|eval\\_metric|pred\\_time\\_val|fit\\_time|pred\\_time\\_val\\_marginal|fit\\_time\\_marginal|stack\\_level|can\\_infer|fit\\_order|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|0|WeightedEnsemble\\_L2|0\\.8684|accuracy|11\\.772661685943604|458\\.7743299007416|0\\.0013399124145507812|0\\.39217352867126465|2|true|14|\n",
    "|1|XGBoost|0\\.8564|accuracy|2\\.2201316356658936|167\\.61618185043335|2\\.2201316356658936|167\\.61618185043335|1|true|11|\n",
    "|2|LightGBMXT|0\\.8548|accuracy|6\\.371644496917725|244\\.1768455505371|6\\.371644496917725|244\\.1768455505371|1|true|4|\n",
    "|3|LightGBMLarge|0\\.848|accuracy|0\\.9738662242889404|46\\.50736856460571|0\\.9738662242889404|46\\.50736856460571|1|true|13|\n",
    "|4|LightGBM|0\\.8436|accuracy|0\\.48837947845458984|33\\.889925479888916|0\\.48837947845458984|33\\.889925479888916|1|true|5|\n",
    "|5|RandomForestEntr|0\\.836|accuracy|0\\.29303932189941406|85\\.54689168930054|0\\.29303932189941406|85\\.54689168930054|1|true|7|\n",
    "|6|RandomForestGini|0\\.836|accuracy|0\\.3043951988220215|79\\.56412959098816|0\\.3043951988220215|79\\.56412959098816|1|true|6|\n",
    "|7|ExtraTreesEntr|0\\.8324|accuracy|0\\.2816450595855713|47\\.66879630088806|0\\.2816450595855713|47\\.66879630088806|1|true|10|\n",
    "|8|ExtraTreesGini|0\\.832|accuracy|0\\.4015657901763916|46\\.59765124320984|0\\.4015657901763916|46\\.59765124320984|1|true|9|\n",
    "|9|NeuralNetTorch|0\\.8308|accuracy|0\\.04175114631652832|614\\.9937784671783|0\\.04175114631652832|614\\.9937784671783|1|true|12|\n",
    "|10|KNeighborsDist|0\\.8272|accuracy|2\\.205679416656494|0\\.08176040649414062|2\\.205679416656494|0\\.08176040649414062|1|true|2|\n",
    "|11|NeuralNetFastAI|0\\.8256|accuracy|0\\.10920500755310059|189\\.042377948761|0\\.10920500755310059|189\\.042377948761|1|true|3|\n",
    "|12|CatBoost|0\\.8168|accuracy|0\\.012778282165527344|201\\.77775239944458|0\\.012778282165527344|201\\.77775239944458|1|true|8|\n",
    "|13|KNeighborsUnif|0\\.8072|accuracy|1\\.5244452953338623|4\\.0205254554748535|1\\.5244452953338623|4\\.0205254554748535|1|true|1|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sB79JFnpwQHn",
    "outputId": "f5151983-2dcf-44b7-9790-b2a0947fe555"
   },
   "outputs": [],
   "source": [
    "predictor_info = predictor.info()\n",
    "predictor_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8zYebqUFxwjH"
   },
   "outputs": [],
   "source": [
    "predictor_info[\"model_info\"][\"WeightedEnsemble_L2\"]\n",
    "predictor_info[\"model_info\"][\"KNeighborsDist\"]\n",
    "predictor_info[\"model_info\"][\"LightGBMXT\"]\n",
    "predictor_info[\"model_info\"][\"XGBoost\"]\n",
    "predictor_info[\"model_info\"][\"LightGBMLarge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "wfq__fd45ZO7",
    "outputId": "63224990-3e48-442b-c327-38d2d5cabd18"
   },
   "outputs": [],
   "source": [
    "predictor_info[\"model_info\"][\"WeightedEnsemble_L2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV_L8y938dGY"
   },
   "source": [
    "<a name=\"feature-engineering-and-selection\"></a>\n",
    "## Feature engineering and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuDLBQlx8eLV"
   },
   "outputs": [],
   "source": [
    "predictor.feature_importance(val, subsample_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYWD5rftG13i"
   },
   "source": [
    "Due to the nature of features, we didn't manage to come up with any reasonable or meaningful features. Thus, we did not do feature engineering. However, AutoGluon enables to determine what features are the most important and should be selected. The results are the following:\n",
    "\n",
    "```\n",
    "                                                  importance    stddev\n",
    " Curricular units 2nd sem (approved)             1.324000e-01  0.021043   \n",
    " Tuition fees up to date                         2.960000e-02  0.009940   \n",
    " Curricular units 1st sem (approved)             1.840000e-02  0.002966   \n",
    " Unemployment rate                               1.080000e-02  0.006870   \n",
    " Course                                          1.000000e-02  0.006633   \n",
    " GDP                                             9.200000e-03  0.004817   \n",
    " Curricular units 2nd sem (grade)                9.200000e-03  0.011713   \n",
    " Curricular units 2nd sem (enrolled)             8.800000e-03  0.007823   \n",
    " Scholarship holder                              7.200000e-03  0.008075   \n",
    " Curricular units 2nd sem (evaluations)          6.000000e-03  0.009055   \n",
    " Inflation rate                                  4.800000e-03  0.007823   \n",
    " Father's occupation                             4.400000e-03  0.006229   \n",
    " Age at enrollment                               3.600000e-03  0.005550   \n",
    " Curricular units 1st sem (evaluations)          2.400000e-03  0.004336   \n",
    " Curricular units 1st sem (enrolled)             2.400000e-03  0.002608   \n",
    " Curricular units 1st sem (without evaluations)  1.600000e-03  0.002966   \n",
    " Gender                                          1.200000e-03  0.003347   \n",
    " Previous qualification                          1.200000e-03  0.003033   \n",
    " Mother's qualification                          8.000000e-04  0.008438   \n",
    " Previous qualification (grade)                  4.000000e-04  0.008877   \n",
    " Mother's occupation                             4.000000e-04  0.004561   \n",
    " Marital status                                  4.000000e-04  0.001673   \n",
    " Daytime/evening attendance                      4.000000e-04  0.002191   \n",
    " Curricular units 2nd sem (credited)             4.000000e-04  0.001673   \n",
    " Father's qualification                          2.220446e-17  0.005831   \n",
    " Nacionality                                     0.000000e+00  0.000000   \n",
    " Debtor                                          0.000000e+00  0.002449   \n",
    " International                                   0.000000e+00  0.000000   \n",
    " Educational special needs                       0.000000e+00  0.000000   \n",
    " Admission grade                                -4.000000e-04  0.004775   \n",
    " Curricular units 2nd sem (without evaluations) -4.000000e-04  0.001673   \n",
    " Application order                              -4.000000e-04  0.003578   \n",
    " Curricular units 1st sem (credited)            -4.000000e-04  0.002608   \n",
    " Application mode                               -8.000000e-04  0.003633   \n",
    " Displaced                                      -1.200000e-03  0.005215   \n",
    " Curricular units 1st sem (grade)               -1.600000e-03  0.006693   \n",
    " id                                             -9.200000e-03  0.005215\n",
    " ```\n",
    "\n",
    " So, there are 24 features that have more impact than 0. We also can see that 12 features are irrelevant according to the retrieved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZzkLE5bI1sE"
   },
   "source": [
    "<a name=\"hyperparameter-tuning\"></a>\n",
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZ9a-oOII0ny"
   },
   "source": [
    "AutoGluon was somewhat successful in hyperparameter tuning:\n",
    "\n",
    "|index|model|weight|model type|used features|hyperparameters|\n",
    "|---|---|---|---|---|---|\n",
    "|1|'KNeighborsDist'|0.38095238095238093|KNN|'Previous qualification (grade)',  'Admission grade',  'Age at enrollment',  'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)',  <br>'Curricular units 1st sem (evaluations)',  'Curricular units 1st sem (approved)',  'Curricular units 1st sem (grade)',  'Curricular units 1st sem (without evaluations)', <br>  'Curricular units 2nd sem (credited)',  'Curricular units 2nd sem (enrolled)',  'Curricular units 2nd sem (evaluations)',  'Curricular units 2nd sem (approved)', <br> 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)',  'Unemployment rate',  'Inflation rate',  'GDP'|'weights': 'distance'|\n",
    "|2|'LightGBMXT'|0.047619047619047616|Light GBM with extra trees|All features|'learning_rate': 0.05, <br> 'extra_trees': True|\n",
    "|3|'XGBoost'|0.38095238095238093|XGBoost|All features|'n_estimators': 10000, <br> 'learning_rate': 0.1, <br> 'n_jobs': -1, <br> 'proc.max_category_levels': 100, <br> 'objective': 'multi:softprob', <br> 'booster': 'gbtree', <br> 'num_class': 3|\n",
    "|4|'LightGBMLarge'|0.19047619047619047|Light GBM |All features|'learning_rate': 0.03, <br> 'num_leaves': 128, <br> 'feature_fraction': 0.9, <br> 'min_data_in_leaf': 3|\n",
    "\n",
    "However, our hypothesis is that hyperparameter tuning could be done better in order to improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvVdlgG7I-U-"
   },
   "source": [
    "<a name=\"model-evaluation\"></a>\n",
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEuxO_vCJCKH"
   },
   "outputs": [],
   "source": [
    "predictor.evaluate(val, detailed_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1TPhRqwKNud"
   },
   "source": [
    "## Model Evaluation Summary\n",
    "\n",
    "* Overall Accuracy: 82.3% — The model correctly classified 82.3% of all cases.\n",
    "* Balanced Accuracy: 78.5% — Reflects balanced performance across all classes.\n",
    "* MCC (Matthews Correlation Coefficient): 0.717 — Indicates strong overall correlation between predictions and true labels.\n",
    "\n",
    "## Class Performance (F1-Scores)\n",
    "\n",
    "* Dropout:\n",
    "  * Precision: 89.4%\n",
    "  * Recall: 82.9%\n",
    "  * F1-Score: 86.0%\n",
    "* Enrolled:\n",
    "  * Precision: 62.7%\n",
    "  * Recall: 62.4%\n",
    "  * F1-Score: 62.6%\n",
    "* Graduate:\n",
    "  * Precision: 85.7%\n",
    "  * Recall: 90.2%\n",
    "  * F1-Score: 87.9%\n",
    "\n",
    "The model performs best on the Graduate class (F1-score: 87.9%) and Dropout class (F1-score: 86.0%).\n",
    "Performance on the Enrolled class is significantly lower (F1-score: 62.6%), suggesting challenges in distinguishing this group but this is probably not that important if the most interesting class is Dropout.\n",
    "\n",
    "## Macro Average:\n",
    "\n",
    "* Precision: 79.3%\n",
    "* Recall: 78.5%\n",
    "* F1-Score: 78.8%\n",
    "\n",
    "## Weighted Average (considering class imbalance):\n",
    "\n",
    "* Precision: 82.4%\n",
    "* Recall: 82.3%\n",
    "* F1-Score: 82.3%\n",
    "\n",
    "## Confusion Matrix Insights:\n",
    "\n",
    "Misclassification occurs more often between Enrolled and other classes, with 390 Enrolled instances misclassified as Graduate and 177 misclassified as Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xELKsenJFaS"
   },
   "source": [
    "<a name=\"results-interpretation\"></a>\n",
    "## Results interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F30MFD7uLXrT"
   },
   "source": [
    "The evaluation of the model shows good overall performance with some notable observations regarding class-specific performance and areas for improvement.\n",
    "\n",
    "* Accuracy (82.3%): The model correctly classified 82.3% of all cases, indicating solid performance overall. However, accuracy alone does not fully reflect class-specific performance, especially in imbalanced datasets.\n",
    "* Balanced Accuracy (78.5%): This metric accounts for imbalanced class distributions and suggests the model maintains reasonably balanced performance across all classes.\n",
    "* MCC (0.717): A high Matthews Correlation Coefficient close to 1 signifies a strong correlation between the predicted and actual labels, confirming the model's robustness.\n",
    "\n",
    "## Class-Specific Insights\n",
    "\n",
    "Dropout-class:\n",
    "\n",
    "* High Precision (89.4%) and Strong Recall (82.9%):\n",
    "The model is highly effective at identifying \"Dropout\" cases while keeping false positives low.\n",
    "* Misclassifications: Some \"Dropout\" cases are confused with \"Enrolled\" (272) and \"Graduate\" (158).\n",
    "\n",
    "Enrolled-class:\n",
    "\n",
    "* Moderate Precision (62.7%) and Recall (62.4%):\n",
    "This is the weakest performing class. The model struggles to identify \"Enrolled\" students, often misclassifying them as \"Graduate\" (390 instances) or \"Dropout\" (177 instances).\n",
    "* Why? The features distinguishing \"Enrolled\" students may overlap with the other two classes, making it difficult for the model to differentiate.\n",
    "\n",
    "Graduate-class:\n",
    "\n",
    "* Strong Recall (90.2%) and Precision (85.7%):\n",
    "The model performs exceptionally well at identifying \"Graduate\" students. Few graduates are misclassified as \"Enrolled\" (287) or \"Dropout\" (70).\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The confusion matrix highlights the following key patterns:\n",
    "\n",
    "* Dropout:\n",
    "  Well-classified overall, but confusion exists with Enrolled students. This may indicate borderline or transitioning cases.\n",
    "* Enrolled:\n",
    "  High misclassification into \"Graduate\" class. This suggests that some features (e.g., academic progress, attendance) for \"Enrolled\" students resemble those of \"Graduate\" students.\n",
    "* Graduate:\n",
    "  This class is the best-performing, showing the clearest separation from the others.\n",
    "\n",
    "## Practical Implications\n",
    "\n",
    "* Strengths: The model is highly reliable in predicting Dropout and Graduate cases, which is critical for interventions targeting students at risk or those nearing graduation.\n",
    "* Weaknesses:\n",
    "The Enrolled class suffers from high misclassification. This could impact decisions aimed at improving retention, as the model may incorrectly identify \"Enrolled\" students as dropouts or graduates.\n",
    "\n",
    "\n",
    "# Future Work (if would happen)\n",
    "\n",
    "* Probably feature engineering even though it's difficult to come up with something meaningful\n",
    "* More robust hyperparameter tuning\n",
    "* Error analysis, i.e. ivestigate misclassified instances (e.g., borderline cases) to understand patterns and refine the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfEJp8yHJNlh"
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJaPKDMKNULv"
   },
   "source": [
    "While we narrowly missed our target accuracy of 0.8350, achieving a final score of 0.8333, the journey has been both rewarding and insightful. We trained numerous models, experimenting with different approaches and refining our techniques along the way. This experience allowed us to apply our skills to a real-world dataset, even though it required minimal preprocessing.\n",
    "\n",
    "It is also worth noting the dynamic nature of the competition. The Kaggle competition was challenging. In fact, the average upward jump in rankings among the current top 10 was 517 spots, demonstrating how fierce and competitive the environment has been.\n",
    "\n",
    "In conclusion, despite falling slightly short of our goal, we made significant progress, and gained valuable experience."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
